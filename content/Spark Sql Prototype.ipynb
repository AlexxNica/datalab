{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%bash\n",
    "java -Xbootclasspath/p:alpn-boot-7.1.3.v20150130.jarr -jar client_deploy.jar --service_root_url https://test-dataproc.sandbox.googleapis.com --project ${PROJECT} rejoin spark-shell-cba39d4a-72c0-4511-a110-e73ced4c9641\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID                                    TYPE     STATUS\n",
      "b1f0efa9-dc15-4487-bc45-e9912e580214  spark    DONE\n",
      "5b0d9360-a069-4044-a1fc-7601111d4f00  pyspark  DONE\n",
      "Hello World.ipynb\n",
      "Spark Sql Prototype.ipynb\n",
      "alpn-boot-7.1.3.v20150130.jar\n",
      "alpn-boot-8.1.3.v20150130.jar\n",
      "client_deploy.jar\n",
      "client_deploy.jar.jre1_8\n",
      "datalab\n",
      "publish.sh\n",
      "tmp\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gcloud beta dataproc jobs list\n",
    "#gcloud beta dataproc jobs kill spark-shell-45b5a8aa-7255-40ff-af0b-0a6c99b9d91b\n",
    "\n",
    "ls\n",
    "\n",
    "%%bash java -Xbootclasspath/p:alpn-boot-7.1.3.v20150130.jarr -jar client_deploy.jar --service_root_url https://test-dataproc.sandbox.googleapis.com --project ${PROJECT} submit --cluster alekse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 2] No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-143fb64c1c6d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     53\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mexecuteSparkCommand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'sqlContext.sql(\"'\u001b[0m\u001b[1;33m+\u001b[0m \u001b[0msqlCommand\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;34m'\").show()'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m \u001b[0mstartInteractive\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-2-143fb64c1c6d>\u001b[0m in \u001b[0;36mstartInteractive\u001b[1;34m()\u001b[0m\n\u001b[0;32m     29\u001b[0m                           \u001b[1;34m'--cluster'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m                           \u001b[1;34m'alekseyv-interact'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m                           'spark-shell'], shell = False, stdout=subprocess.PIPE, stderr=subprocess.PIPE, env = os.environ)\n\u001b[0m\u001b[0;32m     32\u001b[0m     \u001b[1;32mprint\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[1;32mprint\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python2.7/subprocess.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags)\u001b[0m\n\u001b[0;32m    708\u001b[0m                                 \u001b[0mp2cread\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp2cwrite\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    709\u001b[0m                                 \u001b[0mc2pread\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc2pwrite\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 710\u001b[1;33m                                 errread, errwrite)\n\u001b[0m\u001b[0;32m    711\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    712\u001b[0m             \u001b[1;31m# Preserve original exception in case os.close raises.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python2.7/subprocess.pyc\u001b[0m in \u001b[0;36m_execute_child\u001b[1;34m(self, args, executable, preexec_fn, close_fds, cwd, env, universal_newlines, startupinfo, creationflags, shell, to_close, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite)\u001b[0m\n\u001b[0;32m   1333\u001b[0m                         \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1334\u001b[0m                 \u001b[0mchild_exception\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1335\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mchild_exception\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1336\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1337\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: [Errno 2] No such file or directory"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "def rejoinInteractive(jobname):\n",
    "    p = subprocess.Popen(['java',\n",
    "                      '-Xbootclasspath/p:alpn-boot-7.1.3.v20150130.jar',\n",
    "                      '-jar',\n",
    "                      'client_deploy.jar',\n",
    "                      '--service_root_url',\n",
    "                      'https://test-dataproc.sandbox.googleapis.com',\n",
    "                      '--project', 'datalab-spark',\n",
    "                      'rejoin', \n",
    "                      jobname,\n",
    "                      'spark-shell'], shell = False, \n",
    "                     stdout=subprocess.PIPE, stderr=subprocess.PIPE,\n",
    "                     stdin=subprocess.PIPE, env = os.environ)\n",
    "    print p.out\n",
    "    print p.err\n",
    "\n",
    "def startInteractive():\n",
    "    p = subprocess.Popen(['java',\n",
    "                          '-Xbootclasspath/p:alpn-boot-7.1.3.v20150130.jar',\n",
    "                          '-jar',\n",
    "                          'client_deploy.jar',\n",
    "                          '--service_root_url',\n",
    "                          'https://test-dataproc.sandbox.googleapis.com',\n",
    "                          '--project', 'datalab-spark',\n",
    "                          'submit', \n",
    "                          '--cluster', \n",
    "                          'alekseyv-interact', \n",
    "                          'spark-shell'], shell = False, stdout=subprocess.PIPE, stderr=subprocess.PIPE, env = os.environ)\n",
    "    print p.out\n",
    "    print p.err\n",
    "\n",
    "def executeSparkCommand(command):\n",
    "    p = subprocess.Popen(['java',\n",
    "                        '-Xbootclasspath/p:alpn-boot-7.1.3.v20150130.jar',\n",
    "                        '-jar',\n",
    "                        'client_deploy.jar',\n",
    "                        '--service_root_url',\n",
    "                        'https://test-dataproc.sandbox.googleapis.com',\n",
    "                        '--project', 'datalab-spark',\n",
    "                        'rejoin', \n",
    "                        'spark-shell-79747481-f65a-4d99-bbdf-b1067dcc5ae1', \n",
    "                        'spark-shell'], shell = False, \n",
    "                       stdout=subprocess.PIPE, stderr=subprocess.PIPE,\n",
    "                       stdin=subprocess.PIPE, env = os.environ)\n",
    "    out, err = p.communicate(command + ' \\n')\n",
    "    result = out + err\n",
    "    return result[result.rfind('scala>', 0, result.rfind('scala>')):result.rfind('scala>')]\n",
    "\n",
    "def executeSparkSql(sqlCommand):\n",
    "      return executeSparkCommand('sqlContext.sql(\"'+ sqlCommand.replace('\\n', '') +'\").show()')\n",
    "\n",
    "startInteractive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'jobs': [{u'driverControlFilesUri': u'gs://dataproc-38815733-337d-4be6-bd75-9fc858a3bf57-us/google-cloud-dataproc-metainfo/9fd31970-869c-49c2-810d-28046920f1d8/jobs/b1f0efa9-dc15-4487-bc45-e9912e580214/',\n",
       "   u'driverOutputResourceUri': u'gs://dataproc-38815733-337d-4be6-bd75-9fc858a3bf57-us/google-cloud-dataproc-metainfo/9fd31970-869c-49c2-810d-28046920f1d8/jobs/b1f0efa9-dc15-4487-bc45-e9912e580214/driveroutput',\n",
       "   u'placement': {u'clusterName': u'cluster-1',\n",
       "    u'clusterUuid': u'9fd31970-869c-49c2-810d-28046920f1d8'},\n",
       "   u'reference': {u'jobId': u'b1f0efa9-dc15-4487-bc45-e9912e580214',\n",
       "    u'projectId': u'datalab-spark'},\n",
       "   u'sparkJob': {u'args': [u'1000'],\n",
       "    u'jarFileUris': [u'file:///usr/lib/spark/lib/spark-examples.jar'],\n",
       "    u'loggingConfiguration': {},\n",
       "    u'mainClass': u'org.apache.spark.examples.SparkPi'},\n",
       "   u'status': {u'state': u'DONE',\n",
       "    u'stateStartTime': u'2015-10-02T00:17:02.832Z'},\n",
       "   u'statusHistory': [{u'state': u'PENDING',\n",
       "     u'stateStartTime': u'2015-10-02T00:16:15.693Z'},\n",
       "    {u'state': u'SETUP_DONE', u'stateStartTime': u'2015-10-02T00:16:15.837Z'},\n",
       "    {u'details': u'Agent reported job success',\n",
       "     u'state': u'RUNNING',\n",
       "     u'stateStartTime': u'2015-10-02T00:16:20.852Z'}]},\n",
       "  {u'driverControlFilesUri': u'gs://dataproc-38815733-337d-4be6-bd75-9fc858a3bf57-us/google-cloud-dataproc-metainfo/9fd31970-869c-49c2-810d-28046920f1d8/jobs/5b0d9360-a069-4044-a1fc-7601111d4f00/',\n",
       "   u'driverOutputResourceUri': u'gs://dataproc-38815733-337d-4be6-bd75-9fc858a3bf57-us/google-cloud-dataproc-metainfo/9fd31970-869c-49c2-810d-28046920f1d8/jobs/5b0d9360-a069-4044-a1fc-7601111d4f00/driveroutput',\n",
       "   u'placement': {u'clusterName': u'cluster-1',\n",
       "    u'clusterUuid': u'9fd31970-869c-49c2-810d-28046920f1d8'},\n",
       "   u'pysparkJob': {u'loggingConfiguration': {},\n",
       "    u'mainPythonFileUri': u'gs://dataproc-38815733-337d-4be6-bd75-9fc858a3bf57-us/google-cloud-dataproc-staging/9fd31970-869c-49c2-810d-28046920f1d8/hello-world.py'},\n",
       "   u'reference': {u'jobId': u'5b0d9360-a069-4044-a1fc-7601111d4f00',\n",
       "    u'projectId': u'datalab-spark'},\n",
       "   u'status': {u'state': u'DONE',\n",
       "    u'stateStartTime': u'2015-10-02T00:14:27.968Z'},\n",
       "   u'statusHistory': [{u'state': u'PENDING',\n",
       "     u'stateStartTime': u'2015-10-02T00:13:44.436Z'},\n",
       "    {u'state': u'SETUP_DONE', u'stateStartTime': u'2015-10-02T00:13:44.604Z'},\n",
       "    {u'details': u'Agent reported job success',\n",
       "     u'state': u'RUNNING',\n",
       "     u'stateStartTime': u'2015-10-02T00:13:49.799Z'}]}]}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gcp\n",
    "import gcp._context\n",
    "import gcp._util\n",
    "\n",
    "def jobs_list(context):\n",
    "    url = 'https://dataproc.googleapis.com/v1beta1/projects/' + context.project_id + '/jobs'\n",
    "    \n",
    "    return gcp._util.Http.request(url, '', credentials=context.credentials)\n",
    "\n",
    "jobs_list(gcp.Context.default())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.core.magic import (register_line_magic, register_cell_magic,\n",
    "                                register_line_cell_magic)\n",
    "\n",
    "@register_cell_magic\n",
    "def spark_sql(line, cell):\n",
    "    print executeSparkSql(cell)\n",
    "    \n",
    "@register_cell_magic\n",
    "def spark_scala(line, cell):\n",
    "    print executeSparkCommand(cell)\n",
    "\n",
    "del spark_sql,spark_scala"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scala> val df = sqlContext.read.json(\"gs://alekseyv-test/people.json\") \n",
      "15/10/09 01:09:05 INFO org.apache.hadoop.mapred.FileInputFormat: Total input paths to process : 1\n",
      "df: org.apache.spark.sql.DataFrame = [age: bigint, name: string]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%spark_scala\n",
    "val df = sqlContext.read.json(\"gs://alekseyv-test/people.json\")\n",
    "#create temporary table people_json using org.apache.spark.sql.json options (path \"gs://alekseyv-test/people.json\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scala> df.registerTempTable(\"people\") \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%spark_scala\n",
    "df.registerTempTable(\"people\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scala> sqlContext.sql(\"select * from people\").show() \n",
      "15/10/09 01:10:53 INFO hive.ql.parse.ParseDriver: Parsing command: select * from people\n",
      "15/10/09 01:10:53 INFO hive.ql.parse.ParseDriver: Parse Completed\n",
      "15/10/09 01:10:53 INFO org.apache.hadoop.mapred.FileInputFormat: Total input paths to process : 1\n",
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%spark_sql\n",
    "\n",
    "select * from people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scala> sqlContext.sql(\"select sum(age) as total_age from people\").show() \n",
      "15/10/09 01:12:34 INFO hive.ql.parse.ParseDriver: Parsing command: select sum(age) as total_age from people\n",
      "15/10/09 01:12:34 INFO hive.ql.parse.ParseDriver: Parse Completed\n",
      "15/10/09 01:12:34 INFO org.apache.hadoop.mapred.FileInputFormat: Total input paths to process : 1\n",
      "+---------+\n",
      "|total_age|\n",
      "+---------+\n",
      "|       49|\n",
      "+---------+\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%spark_sql\n",
    "\n",
    "select sum(age) as total_age from people"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
