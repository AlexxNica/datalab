{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PROJECT=datalab-spark\n",
    "export CLOUDSDK_API_ENDPOINT_OVERRIDES_DATAPROC=https://test-dataproc.sandbox.googleapis.com/\n",
    "\n",
    "gcloud beta dataproc --project ${PROJECT} clusters create ${USER}-interact --zone us-central1-f --image-version 0.2\n",
    "\n",
    "blaze build java/com/google/cloud/hadoop/services/opensource/client:client_deploy.jar\n",
    "\n",
    "java -Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=n,address=5005 -Xbootclasspath/p:alpn-boot-8.1.3.v20150130.jar -jar ~/client_deploy.jar --service_root_url https://test-dataproc.sandbox.googleapis.com --project ${PROJECT} submit --cluster ${USER}-interact spark-shell\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2:29:41 IN\n"
     ]
    }
   ],
   "source": [
    "import gcp\n",
    "import gcp._context\n",
    "import gcp._util\n",
    "\n",
    "import sys\n",
    "import time\n",
    "import grpc.framework.face.exceptions\n",
    "from grpc.beta import implementations\n",
    "from grpc.beta.interfaces import StatusCode\n",
    "from gcp.spark.bytestream_pb2 import beta_create_ByteStream_stub\n",
    "from gcp.spark.bytestream_pb2 import ReadRequest\n",
    "from gcp.spark.bytestream_pb2 import ReadResponse\n",
    "from gcp.spark.bytestream_pb2 import WriteRequest\n",
    "from gcp.spark.bytestream_pb2 import WriteResponse\n",
    "from gcp.spark.bytestream_pb2 import QueryWriteStatusRequest\n",
    "from gcp.spark.bytestream_pb2 import QueryWriteStatusResponse\n",
    "from oauth2client.client import GoogleCredentials\n",
    "from grpc.beta.implementations import ClientCredentials\n",
    "\n",
    "class ByteStreamClient(object):\n",
    "  _DEFAULT_HOST = \"test-dataproc.sandbox.googleapis.com\";\n",
    "  _DEFAULT_PORT = 443;\n",
    "\n",
    "  _JOB_ID = 'spark-shell-eaa8578e-8271-4ce8-9a4b-905d962af150'\n",
    "  _CLUSTER_ID = '2225e423-ce11-4cb1-851c-eb5f3a5de1cd'\n",
    "  _TIMEOUT_SECONDS = 10\n",
    "  _SSL_CERT_FILE = '/etc/ssl/certs/ca-certificates.crt'\n",
    "\n",
    "  def __init__(self):\n",
    "    self.read_index = 0\n",
    "    self.write_index = 0\n",
    "    #self.read()\n",
    "    \n",
    "  def read(self):\n",
    "    if self.read_index == 0:\n",
    "      data = self.read_bytestream(0, 0)\n",
    "      self.read_index += len(data)\n",
    "      return data\n",
    "    else:\n",
    "      # If we read next chunk starting from read_index, and there is no extra data,\n",
    "      # API will throw exception. Therefore starting to read one byte before\n",
    "      # read_index and adjusting response accordingly.\n",
    "      data = self.read_bytestream(0, self.read_index - 1)\n",
    "      self.read_index += (len(data) - 1)\n",
    "      if len(data) == 0:\n",
    "        return data\n",
    "      else:\n",
    "        return data[1:]\n",
    "  \n",
    "  def write(self, data):\n",
    "    if not data.endswith(\"\\n\"):\n",
    "      data = data + \"\\n\"\n",
    "    #print(self.write_index)\n",
    "    if self.write_index == 0:\n",
    "      self.write_index = self.query_write_status().committed_size\n",
    "    #print(self.write_index)\n",
    "    self.write_index = self.write_bytestream(data, self.write_index)\n",
    "    #print(self.write_index)\n",
    "    \n",
    "  def executeScala(self, data):\n",
    "    self.write(data)\n",
    "    _max_iterations = 10\n",
    "    iterations = 0\n",
    "    response = \"\"\n",
    "    while iterations < _max_iterations:\n",
    "      result = self.read()\n",
    "      print(result)\n",
    "      response += result\n",
    "      if \"scala>\" in result:\n",
    "        return response\n",
    "      time.sleep(0.2)\n",
    "      iterations+=1\n",
    "    return response\n",
    "    \n",
    "  class MetadataTransformer(object):\n",
    "      \"\"\"Callable class to transform metadata for gRPC requests.\n",
    "      :type client: :class:`.client.Client`\n",
    "      :param client: The client that owns the cluster. Provides authorization and\n",
    "                     user agent.\n",
    "      \"\"\"\n",
    "\n",
    "      def __init__(self, client):\n",
    "          self._credentials = client.credentials\n",
    "          self._user_agent = 'GoogleCloudDataLab/1.0'\n",
    "\n",
    "      def __call__(self, ignored_val):\n",
    "          \"\"\"Adds authorization header to request metadata.\"\"\"\n",
    "          access_token = self._credentials.get_access_token().access_token\n",
    "          return [\n",
    "              ('Authorization', 'Bearer ' + access_token),\n",
    "              ('User-agent', self._user_agent),\n",
    "          ]\n",
    "\n",
    "  def get_certs(self):\n",
    "      \"\"\"Gets the root certificates.\n",
    "      .. note::\n",
    "          This is only called by :func:`make_stub`. For most applications,\n",
    "          a few gRPC stubs (four total, one for each service) will be created\n",
    "          when a :class:`.Client` is created. This function will not likely\n",
    "          be used again while that application is running.\n",
    "          However, it may be worthwhile to cache the output of this function.\n",
    "      :rtype: str\n",
    "      :returns: The root certificates for the current machine.\n",
    "      \"\"\"\n",
    "      with open(ByteStreamClient._SSL_CERT_FILE, mode='rb') as file_obj:\n",
    "          return file_obj.read()\n",
    "\n",
    "  def read_bytestream(self, read_limit, read_offset, do_retry = True):\n",
    "    driverInputResourceUri = u'//test-dataproc.sandbox.googleapis.com/projects/datalab-spark/clusters/%s/jobs/%s/bytestreams/stdin' % (ByteStreamClient._CLUSTER_ID, ByteStreamClient._JOB_ID)\n",
    "    driverOutputResourceUri = u'//test-dataproc.sandbox.googleapis.com/projects/datalab-spark/clusters/%s/jobs/%s/bytestreams/stdout' % (ByteStreamClient._CLUSTER_ID, ByteStreamClient._JOB_ID)\n",
    "\n",
    "    custom_metadata_transformer = ByteStreamClient.MetadataTransformer(gcp.Context.default())\n",
    "\n",
    "    root_certificates = self.get_certs()\n",
    "    client_credentials = implementations.ssl_client_credentials(root_certificates, private_key=None, certificate_chain=None)\n",
    "\n",
    "    try:\n",
    "      channel = implementations.secure_channel(ByteStreamClient._DEFAULT_HOST, ByteStreamClient._DEFAULT_PORT, client_credentials)\n",
    "      stub = beta_create_ByteStream_stub(channel, metadata_transformer=custom_metadata_transformer)\n",
    "      readRequest = ReadRequest(resource_name=driverOutputResourceUri, read_limit=read_limit, read_offset=read_offset)\n",
    "      readResponse = stub.Read(readRequest, ByteStreamClient._TIMEOUT_SECONDS)\n",
    "      #print(readResponse.result())\n",
    "      return readResponse.next().data\n",
    "    except grpc.framework.interfaces.face.face.NetworkError as ex:\n",
    "      if ex.code == StatusCode.UNAUTHENTICATED and do_retry:\n",
    "        gcp.Context.default().credentials.refresh(None)\n",
    "        gcp.Context._global_context = None\n",
    "        return self.read_bytestream(read_limit, read_offset, do_retry = False)\n",
    "      print('network error: ex.code:%s, ex.details:%s' % (ex.code, ex.details))\n",
    "    return None\n",
    "\n",
    "  def query_write_status(self, do_retry = True):\n",
    "    driverInputResourceUri = u'//test-dataproc.sandbox.googleapis.com/projects/datalab-spark/clusters/%s/jobs/%s/bytestreams/stdin' % (ByteStreamClient._CLUSTER_ID, ByteStreamClient._JOB_ID)\n",
    "    custom_metadata_transformer = ByteStreamClient.MetadataTransformer(gcp.Context.default())\n",
    "    root_certificates = self.get_certs()\n",
    "    client_credentials = implementations.ssl_client_credentials(root_certificates, private_key=None, certificate_chain=None)\n",
    "    try:\n",
    "      channel = implementations.secure_channel(ByteStreamClient._DEFAULT_HOST, ByteStreamClient._DEFAULT_PORT, client_credentials)\n",
    "      query_write_status_request = QueryWriteStatusRequest(resource_name=driverInputResourceUri)\n",
    "\n",
    "      stub = beta_create_ByteStream_stub(channel, metadata_transformer=custom_metadata_transformer)\n",
    "      return stub.QueryWriteStatus(query_write_status_request, ByteStreamClient._TIMEOUT_SECONDS)\n",
    "    except grpc.framework.interfaces.face.face.NetworkError as ex:\n",
    "      if ex.code == StatusCode.UNAUTHENTICATED and do_retry:\n",
    "        gcp.Context.default().credentials.refresh(None)\n",
    "        gcp.Context._global_context = None\n",
    "        return self.query_write_status(do_retry = False)\n",
    "      print('network error: ex.code:%s, ex.details:%s' % (ex.code, ex.details))\n",
    "    return None\n",
    "\n",
    "  def write_bytestream(self, string_data, write_offset, do_retry = True):\n",
    "    driverInputResourceUri = u'//test-dataproc.sandbox.googleapis.com/projects/datalab-spark/clusters/%s/jobs/%s/bytestreams/stdin' % (ByteStreamClient._CLUSTER_ID, ByteStreamClient._JOB_ID)\n",
    "    driverOutputResourceUri = u'//test-dataproc.sandbox.googleapis.com/projects/datalab-spark/clusters/%s/jobs/%s/bytestreams/stdout' % (ByteStreamClient._CLUSTER_ID, ByteStreamClient._JOB_ID)\n",
    "\n",
    "    custom_metadata_transformer = ByteStreamClient.MetadataTransformer(gcp.Context.default())\n",
    "\n",
    "    root_certificates = self.get_certs()\n",
    "    client_credentials = implementations.ssl_client_credentials(root_certificates, private_key=None, certificate_chain=None)\n",
    "\n",
    "    try:\n",
    "      channel = implementations.secure_channel(ByteStreamClient._DEFAULT_HOST, ByteStreamClient._DEFAULT_PORT, client_credentials)\n",
    "      stub = beta_create_ByteStream_stub(channel, metadata_transformer=custom_metadata_transformer)\n",
    "      writeRequest = WriteRequest(resource_name=driverInputResourceUri, data = str.encode(string_data), write_offset = write_offset, finish_write = False)\n",
    "\n",
    "      writeResponse = stub.Write([writeRequest], ByteStreamClient._TIMEOUT_SECONDS)\n",
    "      return writeResponse.committed_size\n",
    "\n",
    "    except grpc.framework.interfaces.face.face.NetworkError as ex:\n",
    "      if ex.code == StatusCode.UNAUTHENTICATED and do_retry:\n",
    "        gcp.Context.default().credentials.refresh(None)\n",
    "        gcp.Context._global_context = None\n",
    "        return self.write_bytestream(string_data, write_offset, do_retry = False)\n",
    "      print('network error: ex.code:%s, ex.details:%s' % (ex.code, ex.details)) \n",
    "\n",
    "byteStreamClient = ByteStreamClient()\n",
    "print(byteStreamClient.read_bytestream(10, 10))\n",
    "#print(byteStreamClient.read())\n",
    "#print(byteStreamClient.write(\"sqlContext\\n\"))\n",
    "#print(byteStreamClient.executeScala(\"sqlContext\"))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/23 22:29:41 INFO org.spark-project.jetty.server.Server: jetty-8.y.z-SNAPSHOT\n",
      "16/02/23 22:29:41 INFO org.spark-project.jetty.server.AbstractConnector: Started SocketConnector@0.0.0.0:45303\n",
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /___/ .__/\\_,_/_/ /_/\\_\\   version 1.6.0\n",
      "      /_/\n",
      "\n",
      "Using Scala version 2.10.5 (OpenJDK 64-Bit Server VM, Java 1.8.0_72-internal)\n",
      "Type in expressions to have them evaluated.\n",
      "Type :help for more information.\n",
      "16/02/23 22:29:49 INFO akka.event.slf4j.Slf4jLogger: Slf4jLogger started\n",
      "16/02/23 22:29:49 INFO Remoting: Starting remoting\n",
      "16/02/23 22:29:49 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@10.240.0.7:51132]\n",
      "16/02/23 22:29:50 INFO org.spark-project.jetty.server.Server: jetty-8.y.z-SNAPSHOT\n",
      "16/02/23 22:29:50 INFO org.spark-project.jetty.server.AbstractConnector: Started SelectChannelConnector@0.0.0.0:4040\n",
      "16/02/23 22:29:50 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at alekseyv-interact2-m/10.240.0.7:8032\n",
      "16/02/23 22:29:55 INFO org.apache.hadoop.yarn.client.api.impl.YarnClientImpl: Submitted application application_1456266360760_0001\n",
      "Spark context available as sc.\n",
      "ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/etc/hive/conf.dist/ivysettings.xml will be used\n",
      "16/02/23 22:30:03 INFO DependencyResolver: ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/etc/hive/conf.dist/ivysettings.xml will be used\n",
      "16/02/23 22:30:03 INFO org.apache.hadoop.hive.metastore.HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore\n",
      "16/02/23 22:30:03 INFO org.apache.hadoop.hive.metastore.ObjectStore: ObjectStore, initialize called\n",
      "16/02/23 22:30:03 INFO DataNucleus.Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored\n",
      "16/02/23 22:30:03 INFO DataNucleus.Persistence: Property datanucleus.cache.level2 unknown - will be ignored\n",
      "16/02/23 22:30:06 INFO org.apache.hadoop.hive.metastore.ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes=\"Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order\"\n",
      "16/02/23 22:30:08 INFO DataNucleus.Datastore: The class \"org.apache.hadoop.hive.metastore.model.MFieldSchema\" is tagged as \"embedded-only\" so does not have its own datastore table.\n",
      "16/02/23 22:30:08 INFO DataNucleus.Datastore: The class \"org.apache.hadoop.hive.metastore.model.MOrder\" is tagged as \"embedded-only\" so does not have its own datastore table.\n",
      "16/02/23 22:30:10 INFO DataNucleus.Datastore: The class \"org.apache.hadoop.hive.metastore.model.MFieldSchema\" is tagged as \"embedded-only\" so does not have its own datastore table.\n",
      "16/02/23 22:30:10 INFO DataNucleus.Datastore: The class \"org.apache.hadoop.hive.metastore.model.MOrder\" is tagged as \"embedded-only\" so does not have its own datastore table.\n",
      "16/02/23 22:30:11 INFO org.apache.hadoop.hive.metastore.MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY\n",
      "16/02/23 22:30:11 INFO org.apache.hadoop.hive.metastore.ObjectStore: Initialized ObjectStore\n",
      "16/02/23 22:30:12 WARN org.apache.hadoop.hive.metastore.ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0\n",
      "16/02/23 22:30:12 WARN org.apache.hadoop.hive.metastore.ObjectStore: Failed to get database default, returning NoSuchObjectException\n",
      "16/02/23 22:30:12 INFO org.apache.hadoop.hive.metastore.HiveMetaStore: Added admin role in metastore\n",
      "16/02/23 22:30:12 INFO org.apache.hadoop.hive.metastore.HiveMetaStore: Added public role in metastore\n",
      "16/02/23 22:30:12 INFO org.apache.hadoop.hive.metastore.HiveMetaStore: No user is added in admin role, since config is empty\n",
      "16/02/23 22:30:12 INFO org.apache.hadoop.hive.metastore.HiveMetaStore: 0: get_all_databases\n",
      "16/02/23 22:30:12 INFO org.apache.hadoop.hive.metastore.HiveMetaStore.audit: ugi=root\tip=unknown-ip-addr\tcmd=get_all_databases\t\n",
      "16/02/23 22:30:12 INFO org.apache.hadoop.hive.metastore.HiveMetaStore: 0: get_functions: db=default pat=*\n",
      "16/02/23 22:30:12 INFO org.apache.hadoop.hive.metastore.HiveMetaStore.audit: ugi=root\tip=unknown-ip-addr\tcmd=get_functions: db=default pat=*\t\n",
      "16/02/23 22:30:12 INFO DataNucleus.Datastore: The class \"org.apache.hadoop.hive.metastore.model.MResourceUri\" is tagged as \"embedded-only\" so does not have its own datastore table.\n",
      "16/02/23 22:30:12 INFO org.apache.hadoop.hive.ql.session.SessionState: Created HDFS directory: /tmp/hive/root\n",
      "16/02/23 22:30:12 INFO org.apache.hadoop.hive.ql.session.SessionState: Created local directory: /tmp/root\n",
      "16/02/23 22:30:12 INFO org.apache.hadoop.hive.ql.session.SessionState: Created local directory: /tmp/7b2661c4-604f-4bbe-b4ea-613f3d93691b_resources\n",
      "16/02/23 22:30:12 INFO org.apache.hadoop.hive.ql.session.SessionState: Created HDFS directory: /tmp/hive/root/7b2661c4-604f-4bbe-b4ea-613f3d93691b\n",
      "16/02/23 22:30:12 INFO org.apache.hadoop.hive.ql.session.SessionState: Created local directory: /tmp/root/7b2661c4-604f-4bbe-b4ea-613f3d93691b\n",
      "16/02/23 22:30:12 INFO org.apache.hadoop.hive.ql.session.SessionState: Created HDFS directory: /tmp/hive/root/7b2661c4-604f-4bbe-b4ea-613f3d93691b/_tmp_space.db\n",
      "ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/etc/hive/conf.dist/ivysettings.xml will be used\n",
      "16/02/23 22:30:13 INFO DependencyResolver: ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/etc/hive/conf.dist/ivysettings.xml will be used\n",
      "16/02/23 22:30:13 INFO hive.metastore: Trying to connect to metastore with URI thrift://alekseyv-interact2-m:9083\n",
      "16/02/23 22:30:13 INFO hive.metastore: Connected to metastore.\n",
      "16/02/23 22:30:13 INFO org.apache.hadoop.hive.ql.session.SessionState: Created local directory: /tmp/632bce7c-c23e-49c0-ab99-261e4cb59f83_resources\n",
      "16/02/23 22:30:13 INFO org.apache.hadoop.hive.ql.session.SessionState: Created HDFS directory: /tmp/hive/root/632bce7c-c23e-49c0-ab99-261e4cb59f83\n",
      "16/02/23 22:30:13 INFO org.apache.hadoop.hive.ql.session.SessionState: Created local directory: /tmp/root/632bce7c-c23e-49c0-ab99-261e4cb59f83\n",
      "16/02/23 22:30:13 INFO org.apache.hadoop.hive.ql.session.SessionState: Created HDFS directory: /tmp/hive/root/632bce7c-c23e-49c0-ab99-261e4cb59f83/_tmp_space.db\n",
      "SQL context available as sqlContext.\n",
      "\n",
      "scala> sqlContext\n",
      "res0: org.apache.spark.sql.SQLContext = org.apache.spark.sql.hive.HiveContext@5ebd3708\n",
      "\n",
      "scala> sqlContext\n",
      "res1: org.apache.spark.sql.SQLContext = org.apache.spark.sql.hive.HiveContext@5ebd3708\n",
      "\n",
      "scala> sqlContext\n",
      "res2: org.apache.spark.sql.SQLContext = org.apache.spark.sql.hive.HiveContext@5ebd3708\n",
      "\n",
      "scala> val df = sqlContext.read.json('gs://alekseyv-test/people.json')\n",
      "     | df.registerTempTable('people')\n",
      "<console>:2: error: ')' expected but '(' found.\n",
      "       df.registerTempTable('people')\n",
      "                           ^\n",
      "<console>:2: error: unclosed character literal\n",
      "       df.registerTempTable('people')\n",
      "                                   ^\n",
      "\n",
      "scala> \n",
      "\n",
      "scala> \n",
      "\n",
      "scala> df.registerTempTable('people')\n",
      "<console>:1: error: unclosed character literal\n",
      "       df.registerTempTable('people')\n",
      "                                   ^\n",
      "\n",
      "scala> df.registerTempTable('people')\n",
      "<console>:1: error: unclosed character literal\n",
      "       df.registerTempTable('people')\n",
      "                                   ^\n",
      "\n",
      "scala> df.registerTempTable(\"people\")\n",
      "<console>:26: error: not found: value df\n",
      "              df.registerTempTable(\"people\")\n",
      "              ^\n",
      "\n",
      "scala> \n",
      "\n",
      "scala> \n",
      "\n",
      "scala> val df = sqlContext.read.json(\"gs://alekseyv-test/people.json\")\n",
      "16/02/23 22:36:20 INFO com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase: GHFS version: 1.4.3-hadoop2\n",
      "16/02/23 22:36:21 INFO org.apache.hadoop.mapred.FileInputFormat: Total input paths to process : 1\n",
      "\r",
      "[Stage 0:>                                                          (0 + 2) / 2]\r",
      "                                                                                \r",
      "df: org.apache.spark.sql.DataFrame = [age: bigint, name: string]\n",
      "\n",
      "scala> df.registerTempTable(\"people\")\n",
      "\n",
      "scala> sqlContext.sql(\"select * from people\").show() \n",
      "16/02/23 22:37:22 INFO hive.ql.parse.ParseDriver: Parsing command: select * from people\n",
      "16/02/23 22:37:23 INFO hive.ql.parse.ParseDriver: Parse Completed\n",
      "16/02/23 22:37:23 INFO org.apache.hadoop.mapred.FileInputFormat: Total input paths to process : 1\n",
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n",
      "\n",
      "scala> sqlContext.sql(\"select * from people order by age\").show()\n",
      "16/02/23 22:37:56 INFO hive.ql.parse.ParseDriver: Parsing command: select * from people order by age\n",
      "16/02/23 22:37:56 INFO hive.ql.parse.ParseDriver: Parse Completed\n",
      "16/02/23 22:37:56 INFO org.apache.hadoop.mapred.FileInputFormat: Total input paths to process : 1\n",
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  19| Justin|\n",
      "|  30|   Andy|\n",
      "+----+-------+\n",
      "\n",
      "\n",
      "scala> \n",
      "\n",
      "scala> sqlContext\n",
      "res7: org.apache.spark.sql.SQLContext = org.apache.spark.sql.hive.HiveContext@5ebd3708\n",
      "\n",
      "scala> \n"
     ]
    }
   ],
   "source": [
    "print(byteStreamClient.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(byteStreamClient.write(\"sqlContext\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sqlContext\n",
      "res8: org.apache.spark.sql.SQLContext = org.apache.spark.sql.hive.HiveContext@5ebd3708\n",
      "\n",
      "scala> sqlContext\n",
      "res9: org.apache.spark.sql.SQLContext = org.apache.spark.sql.hive.HiveContext@5ebd3708\n",
      "\n",
      "scala> \n",
      "sqlContext\n",
      "res8: org.apache.spark.sql.SQLContext = org.apache.spark.sql.hive.HiveContext@5ebd3708\n",
      "\n",
      "scala> sqlContext\n",
      "res9: org.apache.spark.sql.SQLContext = org.apache.spark.sql.hive.HiveContext@5ebd3708\n",
      "\n",
      "scala> \n"
     ]
    }
   ],
   "source": [
    "print(byteStreamClient.executeScala(\"sqlContext\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(byteStreamClient.executeScala('val df = sqlContext.read.json(\"gs://alekseyv-test/people.json\")\\n'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(byteStreamClient.executeScala('df.registerTempTable(\"people\")\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sqlContext.sql(\"select * from people order by age desc\").show()\n",
      "16/02/23 22:41:58 INFO hive.ql.parse.ParseDriver: Parsing command: select * from people order by age desc\n",
      "16/02/23 22:41:58 INFO hive.ql.parse.ParseDriver: Parse Completed\n",
      "16/02/23 22:41:58 INFO org.apache.hadoop.mapred.FileInputFormat: Total input paths to process : 1\n",
      "\n",
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "|null|Michael|\n",
      "+----+-------+\n",
      "\n",
      "\n",
      "scala> \n",
      "sqlContext.sql(\"select * from people order by age desc\").show()\n",
      "16/02/23 22:41:58 INFO hive.ql.parse.ParseDriver: Parsing command: select * from people order by age desc\n",
      "16/02/23 22:41:58 INFO hive.ql.parse.ParseDriver: Parse Completed\n",
      "16/02/23 22:41:58 INFO org.apache.hadoop.mapred.FileInputFormat: Total input paths to process : 1\n",
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "|null|Michael|\n",
      "+----+-------+\n",
      "\n",
      "\n",
      "scala> \n"
     ]
    }
   ],
   "source": [
    "print(byteStreamClient.executeScala('sqlContext.sql(\"select * from people order by age desc\").show()\\n'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
