{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named Clusters",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-494706fd2fec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgcp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mgcp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatalab\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_commands\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_commands\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_storage_create\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m   \u001b[1;34m\"\"\" Create one or more buckets. \"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/gcp/datalab/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_bigquery\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_chart\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0m_dataproc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_extension\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_modules\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/gcp/datalab/_dataproc.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mgcp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataproc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclusters\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mClusters\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mClusters\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_commands\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_html\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named Clusters"
     ]
    }
   ],
   "source": [
    "import gcp.storage\n",
    "import gcp.datalab._commands as _commands\n",
    "\n",
    "def _storage_create(args, _):\n",
    "  \"\"\" Create one or more buckets. \"\"\"\n",
    "  print \"\\n\\n\"\n",
    "  print args\n",
    "\n",
    "parser = _commands.CommandParser(prog='storage', description=\"\"\"\n",
    "Execute various storage-related operations. Use \"%storage <command> -h\"\n",
    "for help on a specific command.\n",
    "\"\"\")\n",
    "  \n",
    "create_parser = parser.subcommand('create', 'Create one or more GCS buckets.')\n",
    "#create_parser.add_argument('-p', '--project', help='The project associated with the objects')\n",
    "#create_parser.add_argument('-b', '--bucket', help='The name of the bucket(s) to create',\n",
    "#                           nargs='+')\n",
    "create_parser_sub = create_parser.subcommand('subcreate', 'Sub buckets.')\n",
    "create_parser_sub.add_argument('-subp', help='The project associated with the objects')\n",
    "create_parser_sub.set_defaults(func=_storage_create)\n",
    "\n",
    "\n",
    "args = parser.parse(\"create subcreate -subp gs://cloud-datalab-samples\")\n",
    "args.func(vars(args), None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataproc list jobs, project:None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Job Id:[f3d53f24-f885-48d9-842a-30a67f6fee5f], Status:[DONE],\n",
       " Job Id:[965725a0-226e-4e51-aede-42818fed8d02], Status:[DONE],\n",
       " Job Id:[655c79a2-6002-478b-8f2c-e95169a865c5], Status:[CANCELLED],\n",
       " Job Id:[alekseyv-spark-sql-proxy-12345-0eeb08ea-bacd-4ea4-b75d-a77e1bf4f30f], Status:[CANCELLED],\n",
       " Job Id:[alekseyv-spark-sql-proxy-12345-eec954bb-4828-40e0-88d7-af5da251e39d], Status:[CANCELLED],\n",
       " Job Id:[alekseyv-spark-sql-proxy-12345-0524fa28-77da-43f3-8142-e5a9350aeb7a], Status:[CANCELLED],\n",
       " Job Id:[alekseyv-spark-sql-proxy-12345-40acdddf-ff3e-4890-b587-6d809a9d1660], Status:[ERROR],\n",
       " Job Id:[alekseyv-spark-sql-proxy-12345-abddf2f0-c40c-4fb3-b848-436808712c72], Status:[CANCELLED],\n",
       " Job Id:[alekseyv-spark-sql-proxy-12345-cfbb52b1-d006-426f-86e6-0c3171998264], Status:[CANCELLED],\n",
       " Job Id:[alekseyv-spark-sql-proxy-12345-60930ba0-bbae-482e-b593-3a180da0bf6f], Status:[CANCELLED],\n",
       " Job Id:[alekseyv-spark-sql-proxy-12345-a83ddca9-46ee-47e2-9576-07c8a6555d7d], Status:[CANCELLED],\n",
       " Job Id:[alekseyv-spark-sql-proxy-12345-cf2e9a3a-8de3-465c-9d0b-cea866f97064], Status:[CANCELLED],\n",
       " Job Id:[alekseyv-spark-sql-proxy-12345-03f4d26f-db19-4021-a78a-6c45ad1f33fb], Status:[CANCELLED],\n",
       " Job Id:[alekseyv-spark-sql-proxy-12345-39df3904-9e55-4e49-8197-c70f7d079425], Status:[CANCELLED],\n",
       " Job Id:[alekseyv-spark-sql-proxy-12345-ecc962f5-bf39-4fcd-b680-648f3a59a263], Status:[CANCELLED],\n",
       " Job Id:[alekseyv-spark-sql-proxy-12345-7139c278-991e-4edb-95f5-a67f3dd90ea1], Status:[CANCELLED],\n",
       " Job Id:[alekseyv-spark-sql-proxy-12345-335feee8-4e61-46a0-9cb9-4afb3c91b698], Status:[CANCELLED],\n",
       " Job Id:[alekseyv-spark-sql-proxy-12345-a03f6153-d6c1-402b-94aa-f5bf596c27cd], Status:[CANCELLED],\n",
       " Job Id:[spark-sql-proxy-12345_abc], Status:[CANCELLED],\n",
       " Job Id:[c963a25f-d2c3-412b-9606-afcac737e40e], Status:[CANCELLED],\n",
       " Job Id:[d2288114-8a4c-43ae-953e-28696336a152], Status:[ERROR],\n",
       " Job Id:[ce66f94b-c442-449a-9299-d903c60f7101], Status:[DONE],\n",
       " Job Id:[21b10f04-835e-4be9-91d5-94fc7836d71e], Status:[ERROR],\n",
       " Job Id:[2b7730b2-cc3e-4ffb-980b-eade96f3363c], Status:[DONE],\n",
       " Job Id:[1c10dac2-98c1-486a-8a01-131415e6c660], Status:[ERROR],\n",
       " Job Id:[fd53a407-54ab-4610-922a-64448620c62f], Status:[DONE],\n",
       " Job Id:[b1f0efa9-dc15-4487-bc45-e9912e580214], Status:[DONE],\n",
       " Job Id:[5b0d9360-a069-4044-a1fc-7601111d4f00], Status:[DONE]]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%dataproc jobs list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'driverControlFilesUri': u'gs://dataproc-38815733-337d-4be6-bd75-9fc858a3bf57-us/google-cloud-dataproc-metainfo/be71321f-e488-435e-9e00-678961c91b3e/jobs/f3d53f24-f885-48d9-842a-30a67f6fee5f/',\n",
       " u'driverOutputResourceUri': u'gs://dataproc-38815733-337d-4be6-bd75-9fc858a3bf57-us/google-cloud-dataproc-metainfo/be71321f-e488-435e-9e00-678961c91b3e/jobs/f3d53f24-f885-48d9-842a-30a67f6fee5f/driveroutput',\n",
       " u'placement': {u'clusterName': u'alekseyv-interact',\n",
       "  u'clusterUuid': u'be71321f-e488-435e-9e00-678961c91b3e'},\n",
       " u'reference': {u'jobId': u'f3d53f24-f885-48d9-842a-30a67f6fee5f',\n",
       "  u'projectId': u'datalab-spark'},\n",
       " u'sparkSqlJob': {u'loggingConfiguration': {},\n",
       "  u'queryList': {u'queries': [u\"create temporary table people_json4 using org.apache.spark.sql.json options (path 'gs://alekseyv-test/people.json');\\nselect * from people_json4;\"]}},\n",
       " u'status': {u'state': u'DONE',\n",
       "  u'stateStartTime': u'2016-02-24T00:49:33.969Z'},\n",
       " u'statusHistory': [{u'state': u'PENDING',\n",
       "   u'stateStartTime': u'2016-02-24T00:48:50.613Z'},\n",
       "  {u'state': u'SETUP_DONE', u'stateStartTime': u'2016-02-24T00:48:50.692Z'},\n",
       "  {u'details': u'Agent reported job success',\n",
       "   u'state': u'RUNNING',\n",
       "   u'stateStartTime': u'2016-02-24T00:48:56.541Z'}]}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gcp.storage\n",
    "import gcp.dataproc._api\n",
    "import gcp.dataproc.clusters._cluster\n",
    "import gcp.dataproc.jobs._job\n",
    "\n",
    "\n",
    "_api = gcp.dataproc._api.Api(gcp.Context.default())\n",
    "\n",
    "clusters = gcp.dataproc.clusters.Clusters()\n",
    "clusters.list()\n",
    "\n",
    "\n",
    "def list_test():\n",
    "  try:\n",
    "    list_info = _api.clusters_list()\n",
    "  except Exception as e:\n",
    "    raise e\n",
    "\n",
    "  clusters = []\n",
    "  cluster_infos = list_info.get('clusters', [])\n",
    "  if len(cluster_infos):\n",
    "    try:\n",
    "      clusters = [gcp.dataproc.clusters._cluster.Cluster(info['clusterName'], info, context=gcp.Context.default()) for info in cluster_infos]\n",
    "    except KeyError:\n",
    "      raise Exception('Unexpected response from server')\n",
    "\n",
    "  return clusters\n",
    "\n",
    "#list_info = _api.clusters_list()\n",
    "#items = list_info.get('clusters', [])\n",
    "#items[0]['clusterName']\n",
    "#[info['clusterName'] for info in list_info]\n",
    "\n",
    "#clusters.create(\"cluster2\", \"us-central1-f\")\n",
    "#_api.clusters_list()\n",
    "#clusters.list()\n",
    "\"\"\"\n",
    "import gcp._util._http\n",
    "try:\n",
    "  _api.clusters_insert(\"cluster2\", \"us-central1-f\", \"datalab-spark\")\n",
    "except gcp._util._http.RequestException as ex:\n",
    "  print ex.content\n",
    "\"\"\"\n",
    "\n",
    "#cluster = gcp.dataproc.clusters.Cluster(\"alekseyv-interact\")\n",
    "#cluster.delete()\n",
    "\n",
    "#clusters.list()\n",
    "#cluster.metadata()\n",
    "\n",
    "gcp.dataproc._api.Api._MAX_RESULTS = 100\n",
    "\n",
    "\n",
    "_api.jobs_list()\n",
    "jobs = gcp.dataproc.jobs.Jobs()\n",
    "#list(jobs)\n",
    "\n",
    "_api.jobs_get(job_id = 'f3d53f24-f885-48d9-842a-30a67f6fee5f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://dataproc-38815733-337d-4be6-bd75-9fc858a3bf57-us/google-cloud-dataproc-metainfo/be71321f-e488-435e-9e00-678961c91b3e/jobs/f3d53f24-f885-48d9-842a-30a67f6fee5f/driveroutput\n",
      "gs://dataproc-38815733-337d-4be6-bd75-9fc858a3bf57-us/google-cloud-dataproc-metainfo/be71321f-e488-435e-9e00-678961c91b3e/jobs/f3d53f24-f885-48d9-842a-30a67f6fee5f/driveroutput.000000000\n",
      "dataproc-38815733-337d-4be6-bd75-9fc858a3bf57-us\n",
      "google-cloud-dataproc-metainfo/be71321f-e488-435e-9e00-678961c91b3e/jobs/f3d53f24-f885-48d9-842a-30a67f6fee5f/driveroutput.000000000\n",
      "10729\n"
     ]
    }
   ],
   "source": [
    "import urllib\n",
    "import gcp._util\n",
    "import gcp.storage\n",
    "\n",
    "url = 'https://dataproc.googleapis.com/v1beta1/projects/datalab-spark/jobs'\n",
    "job = _api.jobs_get(job_id = 'f3d53f24-f885-48d9-842a-30a67f6fee5f')\n",
    "outputResourceUri = 'gs://dataproc-38815733-337d-4be6-bd75-9fc858a3bf57-us/google-cloud-dataproc-metainfo/\\\n",
    "be71321f-e488-435e-9e00-678961c91b3e/jobs/f3d53f24-f885-48d9-842a-30a67f6fee5f/driveroutput.000000000'\n",
    "print(job['driverOutputResourceUri'])\n",
    "print(outputResourceUri)\n",
    "\n",
    "source_bucket, source_key = gcp.storage._bucket.parse_name(outputResourceUri)\n",
    "print(source_bucket)\n",
    "print(source_key)\n",
    "source = gcp.storage.Item(source_bucket, source_key)\n",
    "try:\n",
    "  print(source.metadata().size)\n",
    "  #print(source.read_from())\n",
    "except Exception as ex:\n",
    "  print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named _stream_helper",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-1eadf71e74c8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mgcp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stream_helper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mstream\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgcp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataproc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjobs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stream_helper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStorageObjectSeriesStream\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'f3d53f24-f885-48d9-842a-30a67f6fee5f'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mwhile\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named _stream_helper"
     ]
    }
   ],
   "source": [
    "import gcp.storage._stream_helper\n",
    "\n",
    "stream = gcp.dataproc.jobs._stream_helper.StorageObjectSeriesStream('f3d53f24-f885-48d9-842a-30a67f6fee5f')\n",
    "\n",
    "while True:\n",
    "  data = stream.read(500)\n",
    "  if data == []:\n",
    "    break\n",
    "  print data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"aaa\" in [\"aaa\", \"bbb\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
