{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = \"aaa\"\n",
    "if not data.endswith(\"\\n\"):\n",
    "  data = data + \"\\nb\"\n",
    "print(data)\n",
    "data.index(\"b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "15/12/17 23:38:47 INFO org.spark-project.jetty.server.Server: jetty-8.y.z-SNAPSHOT\n",
      "15/12/17 23:38:47 INFO org.spark-project.jetty.server.AbstractConnector: Started SocketConnector@0.0.0.0:57231\n",
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /___/ .__/\\_,_/_/ /_/\\_\\   version 1.5.2\n",
      "      /_/\n",
      "\n",
      "Using Scala version 2.10.4 (OpenJDK 64-Bit Server VM, Java 1.8.0_66-internal)\n",
      "Type in expressions to have them evaluated.\n",
      "Type :help for more information.\n",
      "15/12/17 23:38:52 INFO akka.event.slf4j.Slf4jLogger: Slf4jLogger started\n",
      "15/12/17 23:38:52 INFO Remoting: Starting remoting\n",
      "15/12/17 23:38:52 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@10.240.0.5:49995]\n",
      "15/12/17 23:38:52 INFO org.spark-project.jetty.server.Server: jetty-8.y.z-SNAPSHOT\n",
      "15/12/17 23:38:52 INFO org.spark-project.jetty.server.AbstractConnector: Started SocketConnector@0.0.0.0:55240\n",
      "15/12/17 23:38:52 INFO org.spark-project.jetty.server.Server: jetty-8.y.z-SNAPSHOT\n",
      "15/12/17 23:38:52 INFO org.spark-project.jetty.server.AbstractConnector: Started SelectChannelConnector@0.0.0.0:4040\n",
      "15/12/17 23:38:52 WARN org.apache.spark.metrics.MetricsSystem: Using default name DAGScheduler for source because spark.app.id is not set.\n",
      "15/12/17 23:38:53 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at alekseyv-interact-m/10.240.0.5:8032\n",
      "15/12/17 23:38:56 INFO org.apache.hadoop.yarn.client.api.impl.YarnClientImpl: Submitted application application_1449632522237_0007\n",
      "Spark context available as sc.\n",
      "ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/etc/hive/conf.dist/ivysettings.xml will be used\n",
      "15/12/17 23:39:01 INFO DependencyResolver: ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/etc/hive/conf.dist/ivysettings.xml will be used\n",
      "15/12/17 23:39:01 INFO hive.metastore: Trying to connect to metastore with URI thrift://alekseyv-interact-m:9083\n",
      "15/12/17 23:39:01 INFO hive.metastore: Connected to metastore.\n",
      "15/12/17 23:39:01 INFO org.apache.hadoop.hive.ql.session.SessionState: Created local directory: /tmp/e6e17628-37f4-4371-a6d6-ae2d5ba6694d_resources\n",
      "15/12/17 23:39:01 INFO org.apache.hadoop.hive.ql.session.SessionState: Created HDFS directory: /tmp/hive/root/e6e17628-37f4-4371-a6d6-ae2d5ba6694d\n",
      "15/12/17 23:39:01 INFO org.apache.hadoop.hive.ql.session.SessionState: Created local directory: /tmp/root/e6e17628-37f4-4371-a6d6-ae2d5ba6694d\n",
      "15/12/17 23:39:01 INFO org.apache.hadoop.hive.ql.session.SessionState: Created HDFS directory: /tmp/hive/root/e6e17628-37f4-4371-a6d6-ae2d5ba6694d/_tmp_space.db\n",
      "ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/etc/hive/conf.dist/ivysettings.xml will be used\n",
      "15/12/17 23:39:02 INFO DependencyResolver: ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/etc/hive/conf.dist/ivysettings.xml will be used\n",
      "15/12/17 23:39:02 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "15/12/17 23:39:02 INFO hive.metastore: Trying to connect to metastore with URI thrift://alekseyv-interact-m:9083\n",
      "15/12/17 23:39:02 INFO hive.metastore: Connected to metastore.\n",
      "15/12/17 23:39:03 WARN org.apache.hadoop.hdfs.shortcircuit.DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.\n",
      "15/12/17 23:39:03 INFO org.apache.hadoop.hive.ql.session.SessionState: Created local directory: /tmp/ccdb845c-93c3-487f-adfb-ed3c8fcc89bb_resources\n",
      "15/12/17 23:39:03 INFO org.apache.hadoop.hive.ql.session.SessionState: Created HDFS directory: /tmp/hive/root/ccdb845c-93c3-487f-adfb-ed3c8fcc89bb\n",
      "15/12/17 23:39:03 INFO org.apache.hadoop.hive.ql.session.SessionState: Created local directory: /tmp/root/ccdb845c-93c3-487f-adfb-ed3c8fcc89bb\n",
      "15/12/17 23:39:03 INFO org.apache.hadoop.hive.ql.session.SessionState: Created HDFS directory: /tmp/hive/root/ccdb845c-93c3-487f-adfb-ed3c8fcc89bb/_tmp_space.db\n",
      "SQL context available as sqlContext.\n",
      "\n",
      "scala> 15/12/17 23:40:08 ERROR org.apache.spark.scheduler.cluster.YarnScheduler: Lost executor 1 on alekseyv-interact-w-0.c.datalab-spark.internal: remote Rpc client disassociated\n",
      "15/12/17 23:40:08 WARN akka.remote.ReliableDeliverySupervisor: Association with remote system [akka.tcp://sparkExecutor@alekseyv-interact-w-0.c.datalab-spark.internal:60583] has failed, address is now gated for [5000] ms. Reason: [Disassociated] \n",
      "15/12/17 23:40:10 ERROR org.apache.spark.scheduler.cluster.YarnScheduler: Lost executor 2 on alekseyv-interact-w-1.c.datalab-spark.internal: remote Rpc client disassociated\n",
      "15/12/17 23:40:10 WARN akka.remote.ReliableDeliverySupervisor: Association with remote system [akka.tcp://sparkExecutor@alekseyv-interact-w-1.c.datalab-spark.internal:33764] has failed, address is now gated for [5000] ms. Reason: [Disassociated] \n",
      "\n",
      "\n",
      "scala> \n",
      "\n",
      "scala> sqlContext\n",
      "res0: org.apache.spark.sql.SQLContext = org.apache.spark.sql.hive.HiveContext@22db12f6\n",
      "\n",
      "scala> test\n",
      "<console>:20: error: package test is not a value\n",
      "              test\n",
      "              ^\n",
      "\n",
      "scala> sqlContext.3\n",
      "<console>:1: error: ';' expected but double literal found.\n",
      "       sqlContext.3\n",
      "                 ^\n",
      "\n",
      "scala> sqlContext.35\n",
      "<console>:1: error: ';' expected but double literal found.\n",
      "       sqlContext.35\n",
      "                 ^\n",
      "\n",
      "scala> sqlContext.356\n",
      "<console>:1: error: ';' expected but double literal found.\n",
      "       sqlContext.356\n",
      "                 ^\n",
      "\n",
      "scala> val df = sqlContext.read.json(\"gs://alekseyv-test/people.json\")\n",
      "15/12/17 23:47:39 INFO com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase: GHFS version: 1.4.3-hadoop2\n",
      "15/12/17 23:47:41 INFO org.apache.hadoop.mapred.FileInputFormat: Total input paths to process : 1\n",
      "df: org.apache.spark.sql.DataFrame = [age: bigint, name: string]\n",
      "\n",
      "scala> val df = sqlContext.read.json(\"gs://alekseyv-test/people.json\")\n",
      "15/12/17 23:48:17 INFO org.apache.hadoop.mapred.FileInputFormat: Total input paths to process : 1\n",
      "df: org.apache.spark.sql.DataFrame = [age: bigint, name: string]\n",
      "\n",
      "scala> df.registerTempTable(\"people\")\n",
      "\n",
      "scala> val df = sqlContext.read.json(\"gs://alekseyv-test/people.json\")\n",
      "15/12/17 23:50:19 INFO org.apache.hadoop.mapred.FileInputFormat: Total input paths to process : 1\n",
      "df: org.apache.spark.sql.DataFrame = [age: bigint, name: string]\n",
      "\n",
      "scala> df.registerTempTable(\"people\")\n",
      "\n",
      "scala> sqlContext.sql('select * from people').show()\n",
      "<console>:1: error: unclosed character literal\n",
      "       sqlContext.sql('select * from people').show()\n",
      "                                           ^\n",
      "\n",
      "scala> val df = sqlContext.read.json(\"gs://alekseyv-test/people.json\")\n",
      "15/12/17 23:50:33 INFO org.apache.hadoop.mapred.FileInputFormat: Total input paths to process : 1\n",
      "df: org.apache.spark.sql.DataFrame = [age: bigint, name: string]\n",
      "\n",
      "scala> df.registerTempTable(\"people\")\n",
      "\n",
      "scala> sqlContext.sql('select * from people;').show()\n",
      "<console>:1: error: unclosed character literal\n",
      "       sqlContext.sql('select * from people;').show()\n",
      "                                            ^\n",
      "<console>:1: error: ')' expected but ';' found.\n",
      "       sqlContext.sql('select * from people;').show()\n",
      "                                           ^\n",
      "\n",
      "scala> val df = sqlContext.read.json(\"gs://alekseyv-test/people.json\")\n",
      "15/12/17 23:51:56 INFO org.apache.hadoop.mapred.FileInputFormat: Total input paths to process : 1\n",
      "df: org.apache.spark.sql.DataFrame = [age: bigint, name: string]\n",
      "\n",
      "scala> df.registerTempTable(\"people\")\n",
      "\n",
      "scala> sqlContext.sql(\"select * from people\").show()\n",
      "15/12/17 23:51:58 INFO hive.ql.parse.ParseDriver: Parsing command: select * from people\n",
      "15/12/17 23:51:59 INFO hive.ql.parse.ParseDriver: Parse Completed\n",
      "15/12/17 23:51:59 INFO org.apache.hadoop.mapred.FileInputFormat: Total input paths to process : 1\n",
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n",
      "\n",
      "scala> sqlContext.sql(\"select * from people\").show()\n",
      "15/12/17 23:52:22 INFO hive.ql.parse.ParseDriver: Parsing command: select * from people\n",
      "15/12/17 23:52:22 INFO hive.ql.parse.ParseDriver: Parse Completed\n",
      "15/12/17 23:52:23 INFO org.apache.hadoop.mapred.FileInputFormat: Total input paths to process : 1\n",
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n",
      "\n",
      "scala> sqlContext.sql(\"select * from people\").show()\n",
      "15/12/17 23:52:43 INFO hive.ql.parse.ParseDriver: Parsing command: select * from people\n",
      "15/12/17 23:52:43 INFO hive.ql.parse.ParseDriver: Parse Completed\n",
      "15/12/17 23:52:43 INFO org.apache.hadoop.mapred.FileInputFormat: Total input paths to process : 1\n",
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n",
      "\n",
      "scala> sqlContext.sql(\"select * from people\").show()\n",
      "15/12/17 23:53:03 INFO hive.ql.parse.ParseDriver: Parsing command: select * from people\n",
      "15/12/17 23:53:03 INFO hive.ql.parse.ParseDriver: Parse Completed\n",
      "15/12/17 23:53:03 INFO org.apache.hadoop.mapred.FileInputFormat: Total input paths to process : 1\n",
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n",
      "\n",
      "scala> sqlContext.sql(\"select * from people\").show()\n",
      "15/12/17 23:53:24 INFO hive.ql.parse.ParseDriver: Parsing command: select * from people\n",
      "15/12/17 23:53:24 INFO hive.ql.parse.ParseDriver: Parse Completed\n",
      "15/12/17 23:53:24 INFO org.apache.hadoop.mapred.FileInputFormat: Total input paths to process : 1\n",
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n",
      "\n",
      "scala> sqlContext.sql(\"select * from people\").show()\n",
      "15/12/17 23:54:00 INFO hive.ql.parse.ParseDriver: Parsing command: select * from people\n",
      "15/12/17 23:54:00 INFO hive.ql.parse.ParseDriver: Parse Completed\n",
      "15/12/17 23:54:00 INFO org.apache.hadoop.mapred.FileInputFormat: Total input paths to process : 1\n",
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n",
      "\n",
      "scala> \n"
     ]
    }
   ],
   "source": [
    "import gcp\n",
    "import gcp._context\n",
    "import gcp._util\n",
    "\n",
    "import sys\n",
    "import time\n",
    "import grpc.framework.face.exceptions\n",
    "from grpc.beta import implementations\n",
    "from grpc.beta.interfaces import StatusCode\n",
    "from gcp.spark.bytestream_pb2 import beta_create_ByteStream_stub\n",
    "from gcp.spark.bytestream_pb2 import ReadRequest\n",
    "from gcp.spark.bytestream_pb2 import ReadResponse\n",
    "from gcp.spark.bytestream_pb2 import WriteRequest\n",
    "from gcp.spark.bytestream_pb2 import WriteResponse\n",
    "from gcp.spark.bytestream_pb2 import QueryWriteStatusRequest\n",
    "from gcp.spark.bytestream_pb2 import QueryWriteStatusResponse\n",
    "from oauth2client.client import GoogleCredentials\n",
    "from grpc.beta.implementations import ClientCredentials\n",
    "\n",
    "class ByteStreamClient(object):\n",
    "  _DEFAULT_HOST = \"test-dataproc.sandbox.googleapis.com\";\n",
    "  _DEFAULT_PORT = 443;\n",
    "\n",
    "  _JOB_ID = 'spark-shell-f7cfb93d-a656-4854-8f9e-93119d5ad409'\n",
    "  _CLUSTER_ID = '39d18ff1-db0d-4305-bfb1-66dfd69e5758'\n",
    "  _TIMEOUT_SECONDS = 10\n",
    "  _SSL_CERT_FILE = '/etc/ssl/certs/ca-certificates.crt'\n",
    "\n",
    "  def __init__(self):\n",
    "    self.read_index = 0\n",
    "    self.write_index = 0\n",
    "    self.read()\n",
    "    \n",
    "  def read(self):\n",
    "    if self.read_index == 0:\n",
    "      data = self.read_bytestream(0, 0)\n",
    "      self.read_index += len(data)\n",
    "      return data\n",
    "    else:\n",
    "      # If we read next chunk starting from read_index, and there is no extra data,\n",
    "      # API will throw exception. Therefore starting to read one byte before\n",
    "      # read_index and adjusting response accordingly.\n",
    "      data = self.read_bytestream(0, self.read_index - 1)\n",
    "      self.read_index += (len(data) - 1)\n",
    "      if len(data) == 0:\n",
    "        return data\n",
    "      else:\n",
    "        return data[1:]\n",
    "  \n",
    "  def write(self, data):\n",
    "    if not data.endswith(\"\\n\"):\n",
    "      data = data + \"\\n\"\n",
    "    #print(self.write_index)\n",
    "    if self.write_index == 0:\n",
    "      self.write_index = self.query_write_status().committed_size\n",
    "    #print(self.write_index)\n",
    "    self.write_index = self.write_bytestream(data, self.write_index)\n",
    "    #print(self.write_index)\n",
    "    \n",
    "  def executeScala(self, data):\n",
    "    self.write(data)\n",
    "    _max_iterations = 10\n",
    "    iterations = 0\n",
    "    response = \"\"\n",
    "    while iterations < _max_iterations:\n",
    "      result = self.read()\n",
    "      print(result)\n",
    "      response += result\n",
    "      if \"scala>\" in result:\n",
    "        return response\n",
    "      time.sleep(0.2)\n",
    "      iterations+=1\n",
    "    return response\n",
    "    \n",
    "  class MetadataTransformer(object):\n",
    "      \"\"\"Callable class to transform metadata for gRPC requests.\n",
    "      :type client: :class:`.client.Client`\n",
    "      :param client: The client that owns the cluster. Provides authorization and\n",
    "                     user agent.\n",
    "      \"\"\"\n",
    "\n",
    "      def __init__(self, client):\n",
    "          self._credentials = client.credentials\n",
    "          self._user_agent = 'GoogleCloudDataLab/1.0'\n",
    "\n",
    "      def __call__(self, ignored_val):\n",
    "          \"\"\"Adds authorization header to request metadata.\"\"\"\n",
    "          access_token = self._credentials.get_access_token().access_token\n",
    "          return [\n",
    "              ('Authorization', 'Bearer ' + access_token),\n",
    "              ('User-agent', self._user_agent),\n",
    "          ]\n",
    "\n",
    "  def get_certs(self):\n",
    "      \"\"\"Gets the root certificates.\n",
    "      .. note::\n",
    "          This is only called by :func:`make_stub`. For most applications,\n",
    "          a few gRPC stubs (four total, one for each service) will be created\n",
    "          when a :class:`.Client` is created. This function will not likely\n",
    "          be used again while that application is running.\n",
    "          However, it may be worthwhile to cache the output of this function.\n",
    "      :rtype: str\n",
    "      :returns: The root certificates for the current machine.\n",
    "      \"\"\"\n",
    "      with open(ByteStreamClient._SSL_CERT_FILE, mode='rb') as file_obj:\n",
    "          return file_obj.read()\n",
    "\n",
    "  def read_bytestream(self, read_limit, read_offset, do_retry = True):\n",
    "    driverInputResourceUri = u'//test-dataproc.sandbox.googleapis.com/projects/datalab-spark/clusters/%s/jobs/%s/bytestreams/stdin' % (ByteStreamClient._CLUSTER_ID, ByteStreamClient._JOB_ID)\n",
    "    driverOutputResourceUri = u'//test-dataproc.sandbox.googleapis.com/projects/datalab-spark/clusters/%s/jobs/%s/bytestreams/stdout' % (ByteStreamClient._CLUSTER_ID, ByteStreamClient._JOB_ID)\n",
    "\n",
    "    custom_metadata_transformer = ByteStreamClient.MetadataTransformer(gcp.Context.default())\n",
    "\n",
    "    root_certificates = self.get_certs()\n",
    "    client_credentials = implementations.ssl_client_credentials(root_certificates, private_key=None, certificate_chain=None)\n",
    "\n",
    "    try:\n",
    "      channel = implementations.secure_channel(ByteStreamClient._DEFAULT_HOST, ByteStreamClient._DEFAULT_PORT, client_credentials)\n",
    "      stub = beta_create_ByteStream_stub(channel, metadata_transformer=custom_metadata_transformer)\n",
    "      readRequest = ReadRequest(resource_name=driverOutputResourceUri, read_limit=read_limit, read_offset=read_offset)\n",
    "      readResponse = stub.Read(readRequest, ByteStreamClient._TIMEOUT_SECONDS)\n",
    "      #print(readResponse.result())\n",
    "      return readResponse.next().data\n",
    "    except grpc.framework.interfaces.face.face.NetworkError as ex:\n",
    "      if ex.code == StatusCode.UNAUTHENTICATED and do_retry:\n",
    "        gcp.Context.default().credentials.refresh(None)\n",
    "        gcp.Context._global_context = None\n",
    "        return self.read_bytestream(read_limit, read_offset, do_retry = False)\n",
    "      print('network error: ex.code:%s, ex.details:%s' % (ex.code, ex.details))\n",
    "    return None\n",
    "\n",
    "  def query_write_status(self, do_retry = True):\n",
    "    driverInputResourceUri = u'//test-dataproc.sandbox.googleapis.com/projects/datalab-spark/clusters/%s/jobs/%s/bytestreams/stdin' % (ByteStreamClient._CLUSTER_ID, ByteStreamClient._JOB_ID)\n",
    "    custom_metadata_transformer = ByteStreamClient.MetadataTransformer(gcp.Context.default())\n",
    "    root_certificates = self.get_certs()\n",
    "    client_credentials = implementations.ssl_client_credentials(root_certificates, private_key=None, certificate_chain=None)\n",
    "    try:\n",
    "      channel = implementations.secure_channel(ByteStreamClient._DEFAULT_HOST, ByteStreamClient._DEFAULT_PORT, client_credentials)\n",
    "      query_write_status_request = QueryWriteStatusRequest(resource_name=driverInputResourceUri)\n",
    "\n",
    "      stub = beta_create_ByteStream_stub(channel, metadata_transformer=custom_metadata_transformer)\n",
    "      return stub.QueryWriteStatus(query_write_status_request, ByteStreamClient._TIMEOUT_SECONDS)\n",
    "    except grpc.framework.interfaces.face.face.NetworkError as ex:\n",
    "      if ex.code == StatusCode.UNAUTHENTICATED and do_retry:\n",
    "        gcp.Context.default().credentials.refresh(None)\n",
    "        gcp.Context._global_context = None\n",
    "        return self.query_write_status(do_retry = False)\n",
    "      print('network error: ex.code:%s, ex.details:%s' % (ex.code, ex.details))\n",
    "    return None\n",
    "\n",
    "  def write_bytestream(self, string_data, write_offset, do_retry = True):\n",
    "    driverInputResourceUri = u'//test-dataproc.sandbox.googleapis.com/projects/datalab-spark/clusters/%s/jobs/%s/bytestreams/stdin' % (ByteStreamClient._CLUSTER_ID, ByteStreamClient._JOB_ID)\n",
    "    driverOutputResourceUri = u'//test-dataproc.sandbox.googleapis.com/projects/datalab-spark/clusters/%s/jobs/%s/bytestreams/stdout' % (ByteStreamClient._CLUSTER_ID, ByteStreamClient._JOB_ID)\n",
    "\n",
    "    custom_metadata_transformer = ByteStreamClient.MetadataTransformer(gcp.Context.default())\n",
    "\n",
    "    root_certificates = self.get_certs()\n",
    "    client_credentials = implementations.ssl_client_credentials(root_certificates, private_key=None, certificate_chain=None)\n",
    "\n",
    "    try:\n",
    "      channel = implementations.secure_channel(ByteStreamClient._DEFAULT_HOST, ByteStreamClient._DEFAULT_PORT, client_credentials)\n",
    "      stub = beta_create_ByteStream_stub(channel, metadata_transformer=custom_metadata_transformer)\n",
    "      writeRequest = WriteRequest(resource_name=driverInputResourceUri, data = str.encode(string_data), write_offset = write_offset, finish_write = False)\n",
    "\n",
    "      writeResponse = stub.Write([writeRequest], ByteStreamClient._TIMEOUT_SECONDS)\n",
    "      return writeResponse.committed_size\n",
    "\n",
    "    except grpc.framework.interfaces.face.face.NetworkError as ex:\n",
    "      if ex.code == StatusCode.UNAUTHENTICATED and do_retry:\n",
    "        gcp.Context.default().credentials.refresh(None)\n",
    "        gcp.Context._global_context = None\n",
    "        return self.write_bytestream(string_data, write_offset, do_retry = False)\n",
    "      print('network error: ex.code:%s, ex.details:%s' % (ex.code, ex.details)) \n",
    "\n",
    "#gcp.Context.default().credentials.refresh(None)\n",
    "#gcp.Context._global_context = None\n",
    "\n",
    "#byteStreamClient = ByteStreamClient()\n",
    "\n",
    "#print(byteStreamClient.executeScala(\"val df = sqlContext.read.json(\\\"gs://alekseyv-test/people.json\\\")\"))\n",
    "#print(byteStreamClient.executeScala(\"df.registerTempTable(\\\"people\\\")\"))\n",
    "#print(byteStreamClient.executeScala(\"sqlContext.sql(\\\"select * from people\\\").show()\"))\n",
    "#print(byteStreamClient.read())\n",
    "#print(byteStreamClient.read())\n",
    "#print(byteStreamClient.read())\n",
    "#print(byteStreamClient.query_write_status())\n",
    "#print(byteStreamClient.write_bytestream(\"sqlContext \\n\\n\", byteStreamClient.query_write_status().committed_size))\n",
    "#print(byteStreamClient.read_bytestream(0,0))\n",
    "#def read_bytestream(self, read_limit, read_offset, do_retry = True):\n",
    "print(byteStreamClient.write(\"sqlContext\\n\"))\n",
    "print(byteStreamClient.read_bytestream(0,0))\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
