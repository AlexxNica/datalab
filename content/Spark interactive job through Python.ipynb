{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "import gcp\n",
    "import gcp._context\n",
    "import gcp._util\n",
    "\n",
    "_ENDPOINT = 'https://test-dataproc.sandbox.googleapis.com'\n",
    "\n",
    "def jobs_list(context):\n",
    "    print _ENDPOINT\n",
    "    url = _ENDPOINT + '/v1beta1/projects/' + context.project_id + '/jobs/'\n",
    "    print url\n",
    "    return gcp._util.Http.request(url, '', credentials=context.credentials)\n",
    "  \n",
    "def jobs_submit(context):\n",
    "    url = _ENDPOINT + '/v1beta1/projects/' + context.project_id + '/jobs:submit'\n",
    "    \n",
    "    return gcp._util.Http.request(url, '', credentials=context.credentials)  \n",
    "  \n",
    "_JOBS_PATH = '/projects/%s/jobs/%s'\n",
    "  \n",
    "def jobs_get(job_id, context):\n",
    "  \"\"\"Issues a request to retrieve information about a job.\n",
    "  Args:\n",
    "    job_id: the id of the job\n",
    "    project_id: the project id to use to fetch the results; use None for the default project.\n",
    "  Returns:\n",
    "    A parsed result object.\n",
    "  Raises:\n",
    "    Exception if there is an error performing the operation.\n",
    "  \"\"\"\n",
    "  url = _ENDPOINT + '/v1beta1/projects/'+ context.project_id + '/jobs/' + job_id\n",
    "  \n",
    "  return gcp._util.Http.request(url, '', credentials=context.credentials)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://test-dataproc.sandbox.googleapis.com\n",
      "https://test-dataproc.sandbox.googleapis.com/v1beta1/projects/datalab-spark/jobs/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{u'jobs': [{u'driverControlFilesUri': u'gs://dataproc-9c5a7adf-7cdb-4eb2-b42d-d81754653069-us/google-cloud-dataproc-metainfo/39d18ff1-db0d-4305-bfb1-66dfd69e5758/jobs/spark-shell-df1d0b8a-1a7b-4004-a9d0-018a6992da83/',\n",
       "   u'driverInputResourceUri': u'//test-dataproc.sandbox.googleapis.com/projects/datalab-spark/clusters/39d18ff1-db0d-4305-bfb1-66dfd69e5758/jobs/spark-shell-df1d0b8a-1a7b-4004-a9d0-018a6992da83/bytestreams/stdin',\n",
       "   u'driverOutputResourceUri': u'//test-dataproc.sandbox.googleapis.com/projects/datalab-spark/clusters/39d18ff1-db0d-4305-bfb1-66dfd69e5758/jobs/spark-shell-df1d0b8a-1a7b-4004-a9d0-018a6992da83/bytestreams/stdout',\n",
       "   u'interactive': True,\n",
       "   u'placement': {u'clusterName': u'alekseyv-interact',\n",
       "    u'clusterUuid': u'39d18ff1-db0d-4305-bfb1-66dfd69e5758'},\n",
       "   u'reference': {u'jobId': u'spark-shell-df1d0b8a-1a7b-4004-a9d0-018a6992da83',\n",
       "    u'projectId': u'datalab-spark'},\n",
       "   u'sparkJob': {u'loggingConfiguration': {},\n",
       "    u'mainClass': u'org.apache.spark.repl.Main'},\n",
       "   u'status': {u'state': u'RUNNING',\n",
       "    u'stateStartTime': u'2015-12-09T03:43:14.273Z'},\n",
       "   u'statusHistory': [{u'state': u'PENDING',\n",
       "     u'stateStartTime': u'2015-12-09T03:43:06.974Z'},\n",
       "    {u'state': u'SETUP_DONE',\n",
       "     u'stateStartTime': u'2015-12-09T03:43:07.306Z'}]},\n",
       "  {u'driverControlFilesUri': u'gs://dataproc-9c5a7adf-7cdb-4eb2-b42d-d81754653069-us/google-cloud-dataproc-metainfo/74aa2106-8f97-472e-a593-2b675af68b6a/jobs/spark-shell-e8c4e86b-5ae2-4ed2-b759-ff795b191444/',\n",
       "   u'driverInputResourceUri': u'//test-dataproc.sandbox.googleapis.com/projects/datalab-spark/clusters/74aa2106-8f97-472e-a593-2b675af68b6a/jobs/spark-shell-e8c4e86b-5ae2-4ed2-b759-ff795b191444/bytestreams/stdin',\n",
       "   u'driverOutputResourceUri': u'//test-dataproc.sandbox.googleapis.com/projects/datalab-spark/clusters/74aa2106-8f97-472e-a593-2b675af68b6a/jobs/spark-shell-e8c4e86b-5ae2-4ed2-b759-ff795b191444/bytestreams/stdout',\n",
       "   u'interactive': True,\n",
       "   u'placement': {u'clusterName': u'alekseyv-interact',\n",
       "    u'clusterUuid': u'74aa2106-8f97-472e-a593-2b675af68b6a'},\n",
       "   u'reference': {u'jobId': u'spark-shell-e8c4e86b-5ae2-4ed2-b759-ff795b191444',\n",
       "    u'projectId': u'datalab-spark'},\n",
       "   u'sparkJob': {u'loggingConfiguration': {},\n",
       "    u'mainClass': u'org.apache.spark.repl.Main'},\n",
       "   u'status': {u'details': u'Task was cancelled.',\n",
       "    u'state': u'ERROR',\n",
       "    u'stateStartTime': u'2015-12-09T03:40:17.711Z'},\n",
       "   u'statusHistory': [{u'state': u'PENDING',\n",
       "     u'stateStartTime': u'2015-12-05T00:26:15.465Z'},\n",
       "    {u'state': u'SETUP_DONE', u'stateStartTime': u'2015-12-05T00:26:15.693Z'},\n",
       "    {u'state': u'RUNNING', u'stateStartTime': u'2015-12-05T00:26:21.722Z'}]},\n",
       "  {u'driverControlFilesUri': u'gs://dataproc-9c5a7adf-7cdb-4eb2-b42d-d81754653069-us/google-cloud-dataproc-metainfo/74aa2106-8f97-472e-a593-2b675af68b6a/jobs/spark-shell-9349b56f-34a8-4709-9dfa-d048c8d31bf3/',\n",
       "   u'driverInputResourceUri': u'//test-dataproc.sandbox.googleapis.com/projects/datalab-spark/clusters/74aa2106-8f97-472e-a593-2b675af68b6a/jobs/spark-shell-9349b56f-34a8-4709-9dfa-d048c8d31bf3/bytestreams/stdin',\n",
       "   u'driverOutputResourceUri': u'//test-dataproc.sandbox.googleapis.com/projects/datalab-spark/clusters/74aa2106-8f97-472e-a593-2b675af68b6a/jobs/spark-shell-9349b56f-34a8-4709-9dfa-d048c8d31bf3/bytestreams/stdout',\n",
       "   u'interactive': True,\n",
       "   u'placement': {u'clusterName': u'alekseyv-interact',\n",
       "    u'clusterUuid': u'74aa2106-8f97-472e-a593-2b675af68b6a'},\n",
       "   u'reference': {u'jobId': u'spark-shell-9349b56f-34a8-4709-9dfa-d048c8d31bf3',\n",
       "    u'projectId': u'datalab-spark'},\n",
       "   u'sparkJob': {u'loggingConfiguration': {},\n",
       "    u'mainClass': u'org.apache.spark.repl.Main'},\n",
       "   u'status': {u'state': u'CANCELLED',\n",
       "    u'stateStartTime': u'2015-12-05T00:33:32.621Z'},\n",
       "   u'statusHistory': [{u'state': u'PENDING',\n",
       "     u'stateStartTime': u'2015-12-05T00:25:31.911Z'},\n",
       "    {u'state': u'SETUP_DONE', u'stateStartTime': u'2015-12-05T00:25:32.109Z'},\n",
       "    {u'state': u'RUNNING', u'stateStartTime': u'2015-12-05T00:25:41.487Z'},\n",
       "    {u'state': u'CANCEL_PENDING',\n",
       "     u'stateStartTime': u'2015-12-05T00:33:06.531Z'},\n",
       "    {u'details': u'Agent reported job failure',\n",
       "     u'state': u'CANCEL_STARTED',\n",
       "     u'stateStartTime': u'2015-12-05T00:33:17.538Z'}]},\n",
       "  {u'driverControlFilesUri': u'gs://dataproc-9c5a7adf-7cdb-4eb2-b42d-d81754653069-us/google-cloud-dataproc-metainfo/74aa2106-8f97-472e-a593-2b675af68b6a/jobs/spark-shell-b3c1f038-e564-4561-910e-fd39df314798/',\n",
       "   u'driverInputResourceUri': u'//test-dataproc.sandbox.googleapis.com/projects/datalab-spark/clusters/74aa2106-8f97-472e-a593-2b675af68b6a/jobs/spark-shell-b3c1f038-e564-4561-910e-fd39df314798/bytestreams/stdin',\n",
       "   u'driverOutputResourceUri': u'//test-dataproc.sandbox.googleapis.com/projects/datalab-spark/clusters/74aa2106-8f97-472e-a593-2b675af68b6a/jobs/spark-shell-b3c1f038-e564-4561-910e-fd39df314798/bytestreams/stdout',\n",
       "   u'interactive': True,\n",
       "   u'placement': {u'clusterName': u'alekseyv-interact',\n",
       "    u'clusterUuid': u'74aa2106-8f97-472e-a593-2b675af68b6a'},\n",
       "   u'reference': {u'jobId': u'spark-shell-b3c1f038-e564-4561-910e-fd39df314798',\n",
       "    u'projectId': u'datalab-spark'},\n",
       "   u'sparkJob': {u'loggingConfiguration': {},\n",
       "    u'mainClass': u'org.apache.spark.repl.Main'},\n",
       "   u'status': {u'state': u'CANCELLED',\n",
       "    u'stateStartTime': u'2015-12-05T00:34:27.846Z'},\n",
       "   u'statusHistory': [{u'state': u'PENDING',\n",
       "     u'stateStartTime': u'2015-12-05T00:24:50.571Z'},\n",
       "    {u'state': u'SETUP_DONE', u'stateStartTime': u'2015-12-05T00:24:50.811Z'},\n",
       "    {u'state': u'RUNNING', u'stateStartTime': u'2015-12-05T00:25:02.627Z'},\n",
       "    {u'state': u'CANCEL_PENDING',\n",
       "     u'stateStartTime': u'2015-12-05T00:33:22.646Z'},\n",
       "    {u'details': u'Agent reported job failure',\n",
       "     u'state': u'CANCEL_STARTED',\n",
       "     u'stateStartTime': u'2015-12-05T00:34:04.749Z'}]},\n",
       "  {u'driverControlFilesUri': u'gs://dataproc-9c5a7adf-7cdb-4eb2-b42d-d81754653069-us/google-cloud-dataproc-metainfo/74aa2106-8f97-472e-a593-2b675af68b6a/jobs/spark-shell-eb1a2cfe-a8da-4394-8106-b474bd412dbc/',\n",
       "   u'driverInputResourceUri': u'//test-dataproc.sandbox.googleapis.com/projects/datalab-spark/clusters/74aa2106-8f97-472e-a593-2b675af68b6a/jobs/spark-shell-eb1a2cfe-a8da-4394-8106-b474bd412dbc/bytestreams/stdin',\n",
       "   u'driverOutputResourceUri': u'//test-dataproc.sandbox.googleapis.com/projects/datalab-spark/clusters/74aa2106-8f97-472e-a593-2b675af68b6a/jobs/spark-shell-eb1a2cfe-a8da-4394-8106-b474bd412dbc/bytestreams/stdout',\n",
       "   u'interactive': True,\n",
       "   u'placement': {u'clusterName': u'alekseyv-interact',\n",
       "    u'clusterUuid': u'74aa2106-8f97-472e-a593-2b675af68b6a'},\n",
       "   u'reference': {u'jobId': u'spark-shell-eb1a2cfe-a8da-4394-8106-b474bd412dbc',\n",
       "    u'projectId': u'datalab-spark'},\n",
       "   u'sparkJob': {u'loggingConfiguration': {},\n",
       "    u'mainClass': u'org.apache.spark.repl.Main'},\n",
       "   u'status': {u'state': u'CANCELLED',\n",
       "    u'stateStartTime': u'2015-12-05T00:34:26.438Z'},\n",
       "   u'statusHistory': [{u'state': u'PENDING',\n",
       "     u'stateStartTime': u'2015-12-04T20:06:11.240Z'},\n",
       "    {u'state': u'SETUP_DONE', u'stateStartTime': u'2015-12-04T20:06:11.587Z'},\n",
       "    {u'state': u'RUNNING', u'stateStartTime': u'2015-12-04T20:06:21.989Z'},\n",
       "    {u'state': u'CANCEL_PENDING',\n",
       "     u'stateStartTime': u'2015-12-05T00:33:34.430Z'},\n",
       "    {u'details': u'Agent reported job failure',\n",
       "     u'state': u'CANCEL_STARTED',\n",
       "     u'stateStartTime': u'2015-12-05T00:33:49.863Z'}]},\n",
       "  {u'driverControlFilesUri': u'gs://dataproc-9c5a7adf-7cdb-4eb2-b42d-d81754653069-us/google-cloud-dataproc-metainfo/617b4f4c-e48a-4a66-bfb2-430ba8cbcbd1/jobs/spark-shell-86a07d5c-8c32-44c4-ae56-35b4c1096347/',\n",
       "   u'driverInputResourceUri': u'//test-dataproc.sandbox.googleapis.com/projects/datalab-spark/clusters/617b4f4c-e48a-4a66-bfb2-430ba8cbcbd1/jobs/spark-shell-86a07d5c-8c32-44c4-ae56-35b4c1096347/bytestreams/stdin',\n",
       "   u'driverOutputResourceUri': u'//test-dataproc.sandbox.googleapis.com/projects/datalab-spark/clusters/617b4f4c-e48a-4a66-bfb2-430ba8cbcbd1/jobs/spark-shell-86a07d5c-8c32-44c4-ae56-35b4c1096347/bytestreams/stdout',\n",
       "   u'interactive': True,\n",
       "   u'placement': {u'clusterName': u'alekseyv-interact',\n",
       "    u'clusterUuid': u'617b4f4c-e48a-4a66-bfb2-430ba8cbcbd1'},\n",
       "   u'reference': {u'jobId': u'spark-shell-86a07d5c-8c32-44c4-ae56-35b4c1096347',\n",
       "    u'projectId': u'datalab-spark'},\n",
       "   u'sparkJob': {u'loggingConfiguration': {},\n",
       "    u'mainClass': u'org.apache.spark.repl.Main'},\n",
       "   u'status': {u'details': u'Task was cancelled.',\n",
       "    u'state': u'ERROR',\n",
       "    u'stateStartTime': u'2015-11-26T00:16:26.397Z'},\n",
       "   u'statusHistory': [{u'state': u'PENDING',\n",
       "     u'stateStartTime': u'2015-11-26T00:08:20.593Z'},\n",
       "    {u'state': u'SETUP_DONE', u'stateStartTime': u'2015-11-26T00:08:20.861Z'},\n",
       "    {u'state': u'RUNNING', u'stateStartTime': u'2015-11-26T00:08:26.186Z'}]},\n",
       "  {u'driverControlFilesUri': u'gs://dataproc-9c5a7adf-7cdb-4eb2-b42d-d81754653069-us/google-cloud-dataproc-metainfo/617b4f4c-e48a-4a66-bfb2-430ba8cbcbd1/jobs/spark-shell-1309e13c-e59a-45ac-a38d-39e7137ad54e/',\n",
       "   u'driverInputResourceUri': u'//test-dataproc.sandbox.googleapis.com/projects/datalab-spark/clusters/617b4f4c-e48a-4a66-bfb2-430ba8cbcbd1/jobs/spark-shell-1309e13c-e59a-45ac-a38d-39e7137ad54e/bytestreams/stdin',\n",
       "   u'driverOutputResourceUri': u'//test-dataproc.sandbox.googleapis.com/projects/datalab-spark/clusters/617b4f4c-e48a-4a66-bfb2-430ba8cbcbd1/jobs/spark-shell-1309e13c-e59a-45ac-a38d-39e7137ad54e/bytestreams/stdout',\n",
       "   u'interactive': True,\n",
       "   u'placement': {u'clusterName': u'alekseyv-interact',\n",
       "    u'clusterUuid': u'617b4f4c-e48a-4a66-bfb2-430ba8cbcbd1'},\n",
       "   u'reference': {u'jobId': u'spark-shell-1309e13c-e59a-45ac-a38d-39e7137ad54e',\n",
       "    u'projectId': u'datalab-spark'},\n",
       "   u'sparkJob': {u'loggingConfiguration': {},\n",
       "    u'mainClass': u'org.apache.spark.repl.Main'},\n",
       "   u'status': {u'details': u'Task was cancelled.',\n",
       "    u'state': u'ERROR',\n",
       "    u'stateStartTime': u'2015-11-26T00:16:22.685Z'},\n",
       "   u'statusHistory': [{u'state': u'PENDING',\n",
       "     u'stateStartTime': u'2015-11-26T00:02:16.691Z'},\n",
       "    {u'state': u'SETUP_DONE', u'stateStartTime': u'2015-11-26T00:02:17.280Z'},\n",
       "    {u'state': u'RUNNING', u'stateStartTime': u'2015-11-26T00:02:22.523Z'}]},\n",
       "  {u'driverControlFilesUri': u'gs://dataproc-9c5a7adf-7cdb-4eb2-b42d-d81754653069-us/google-cloud-dataproc-metainfo/0e18e110-3a94-4bf0-8651-8d79f90d4ae1/jobs/spark-shell-c6940cf3-1b7b-4d56-ad9b-8eb8a3505e98/',\n",
       "   u'driverInputResourceUri': u'//test-dataproc.sandbox.googleapis.com/projects/datalab-spark/clusters/0e18e110-3a94-4bf0-8651-8d79f90d4ae1/jobs/spark-shell-c6940cf3-1b7b-4d56-ad9b-8eb8a3505e98/bytestreams/stdin',\n",
       "   u'driverOutputResourceUri': u'//test-dataproc.sandbox.googleapis.com/projects/datalab-spark/clusters/0e18e110-3a94-4bf0-8651-8d79f90d4ae1/jobs/spark-shell-c6940cf3-1b7b-4d56-ad9b-8eb8a3505e98/bytestreams/stdout',\n",
       "   u'interactive': True,\n",
       "   u'placement': {u'clusterName': u'alekseyv-interact',\n",
       "    u'clusterUuid': u'0e18e110-3a94-4bf0-8651-8d79f90d4ae1'},\n",
       "   u'reference': {u'jobId': u'spark-shell-c6940cf3-1b7b-4d56-ad9b-8eb8a3505e98',\n",
       "    u'projectId': u'datalab-spark'},\n",
       "   u'sparkJob': {u'loggingConfiguration': {},\n",
       "    u'mainClass': u'org.apache.spark.repl.Main'},\n",
       "   u'status': {u'details': u'Task was cancelled.',\n",
       "    u'state': u'ERROR',\n",
       "    u'stateStartTime': u'2015-11-25T23:57:20.471Z'},\n",
       "   u'statusHistory': [{u'state': u'PENDING',\n",
       "     u'stateStartTime': u'2015-11-25T23:51:08.988Z'},\n",
       "    {u'state': u'SETUP_DONE', u'stateStartTime': u'2015-11-25T23:51:09.264Z'},\n",
       "    {u'state': u'RUNNING', u'stateStartTime': u'2015-11-25T23:51:20.287Z'}]},\n",
       "  {u'driverControlFilesUri': u'gs://dataproc-9c5a7adf-7cdb-4eb2-b42d-d81754653069-us/google-cloud-dataproc-metainfo/0e18e110-3a94-4bf0-8651-8d79f90d4ae1/jobs/spark-shell-af165cb1-63dc-4e3f-ae58-5054bcf84b26/',\n",
       "   u'driverInputResourceUri': u'//test-dataproc.sandbox.googleapis.com/projects/datalab-spark/clusters/0e18e110-3a94-4bf0-8651-8d79f90d4ae1/jobs/spark-shell-af165cb1-63dc-4e3f-ae58-5054bcf84b26/bytestreams/stdin',\n",
       "   u'driverOutputResourceUri': u'//test-dataproc.sandbox.googleapis.com/projects/datalab-spark/clusters/0e18e110-3a94-4bf0-8651-8d79f90d4ae1/jobs/spark-shell-af165cb1-63dc-4e3f-ae58-5054bcf84b26/bytestreams/stdout',\n",
       "   u'interactive': True,\n",
       "   u'placement': {u'clusterName': u'alekseyv-interact',\n",
       "    u'clusterUuid': u'0e18e110-3a94-4bf0-8651-8d79f90d4ae1'},\n",
       "   u'reference': {u'jobId': u'spark-shell-af165cb1-63dc-4e3f-ae58-5054bcf84b26',\n",
       "    u'projectId': u'datalab-spark'},\n",
       "   u'sparkJob': {u'loggingConfiguration': {},\n",
       "    u'mainClass': u'org.apache.spark.repl.Main'},\n",
       "   u'status': {u'details': u'Task was cancelled.',\n",
       "    u'state': u'ERROR',\n",
       "    u'stateStartTime': u'2015-11-25T23:57:18.949Z'},\n",
       "   u'statusHistory': [{u'state': u'PENDING',\n",
       "     u'stateStartTime': u'2015-11-25T22:57:13.062Z'},\n",
       "    {u'state': u'SETUP_DONE', u'stateStartTime': u'2015-11-25T22:57:13.349Z'},\n",
       "    {u'state': u'RUNNING', u'stateStartTime': u'2015-11-25T22:57:18.737Z'}]},\n",
       "  {u'driverControlFilesUri': u'gs://dataproc-9c5a7adf-7cdb-4eb2-b42d-d81754653069-us/google-cloud-dataproc-metainfo/0e18e110-3a94-4bf0-8651-8d79f90d4ae1/jobs/spark-shell-4572d1d8-5fe7-44e1-a14b-a9b091ca2179/',\n",
       "   u'driverInputResourceUri': u'//test-dataproc.sandbox.googleapis.com/projects/datalab-spark/clusters/0e18e110-3a94-4bf0-8651-8d79f90d4ae1/jobs/spark-shell-4572d1d8-5fe7-44e1-a14b-a9b091ca2179/bytestreams/stdin',\n",
       "   u'driverOutputResourceUri': u'//test-dataproc.sandbox.googleapis.com/projects/datalab-spark/clusters/0e18e110-3a94-4bf0-8651-8d79f90d4ae1/jobs/spark-shell-4572d1d8-5fe7-44e1-a14b-a9b091ca2179/bytestreams/stdout',\n",
       "   u'interactive': True,\n",
       "   u'placement': {u'clusterName': u'alekseyv-interact',\n",
       "    u'clusterUuid': u'0e18e110-3a94-4bf0-8651-8d79f90d4ae1'},\n",
       "   u'reference': {u'jobId': u'spark-shell-4572d1d8-5fe7-44e1-a14b-a9b091ca2179',\n",
       "    u'projectId': u'datalab-spark'},\n",
       "   u'sparkJob': {u'loggingConfiguration': {},\n",
       "    u'mainClass': u'org.apache.spark.repl.Main'},\n",
       "   u'status': {u'state': u'CANCELLED',\n",
       "    u'stateStartTime': u'2015-11-25T22:54:13.533Z'},\n",
       "   u'statusHistory': [{u'state': u'PENDING',\n",
       "     u'stateStartTime': u'2015-11-23T19:34:52.439Z'},\n",
       "    {u'state': u'SETUP_DONE', u'stateStartTime': u'2015-11-23T19:34:52.627Z'},\n",
       "    {u'state': u'RUNNING', u'stateStartTime': u'2015-11-23T19:35:07.726Z'},\n",
       "    {u'state': u'CANCEL_PENDING',\n",
       "     u'stateStartTime': u'2015-11-25T22:53:56.696Z'},\n",
       "    {u'details': u'Agent reported job failure',\n",
       "     u'state': u'CANCEL_STARTED',\n",
       "     u'stateStartTime': u'2015-11-25T22:54:01.672Z'}]},\n",
       "  {u'driverControlFilesUri': u'gs://dataproc-9c5a7adf-7cdb-4eb2-b42d-d81754653069-us/google-cloud-dataproc-metainfo/0e18e110-3a94-4bf0-8651-8d79f90d4ae1/jobs/spark-shell-1c7e6c12-8cd9-4d23-8fc1-011621008d0f/',\n",
       "   u'driverInputResourceUri': u'//test-dataproc.sandbox.googleapis.com/projects/datalab-spark/clusters/0e18e110-3a94-4bf0-8651-8d79f90d4ae1/jobs/spark-shell-1c7e6c12-8cd9-4d23-8fc1-011621008d0f/bytestreams/stdin',\n",
       "   u'driverOutputResourceUri': u'//test-dataproc.sandbox.googleapis.com/projects/datalab-spark/clusters/0e18e110-3a94-4bf0-8651-8d79f90d4ae1/jobs/spark-shell-1c7e6c12-8cd9-4d23-8fc1-011621008d0f/bytestreams/stdout',\n",
       "   u'interactive': True,\n",
       "   u'placement': {u'clusterName': u'alekseyv-interact',\n",
       "    u'clusterUuid': u'0e18e110-3a94-4bf0-8651-8d79f90d4ae1'},\n",
       "   u'reference': {u'jobId': u'spark-shell-1c7e6c12-8cd9-4d23-8fc1-011621008d0f',\n",
       "    u'projectId': u'datalab-spark'},\n",
       "   u'sparkJob': {u'loggingConfiguration': {},\n",
       "    u'mainClass': u'org.apache.spark.repl.Main'},\n",
       "   u'status': {u'state': u'CANCELLED',\n",
       "    u'stateStartTime': u'2015-11-23T19:35:04.136Z'},\n",
       "   u'statusHistory': [{u'state': u'PENDING',\n",
       "     u'stateStartTime': u'2015-11-23T19:26:32.317Z'},\n",
       "    {u'state': u'SETUP_DONE', u'stateStartTime': u'2015-11-23T19:26:32.561Z'},\n",
       "    {u'state': u'RUNNING', u'stateStartTime': u'2015-11-23T19:26:44.754Z'},\n",
       "    {u'state': u'CANCEL_PENDING',\n",
       "     u'stateStartTime': u'2015-11-23T19:34:28.491Z'},\n",
       "    {u'details': u'Agent reported job failure',\n",
       "     u'state': u'CANCEL_STARTED',\n",
       "     u'stateStartTime': u'2015-11-23T19:34:39.877Z'}]},\n",
       "  {u'driverControlFilesUri': u'gs://dataproc-9c5a7adf-7cdb-4eb2-b42d-d81754653069-us/google-cloud-dataproc-metainfo/0e18e110-3a94-4bf0-8651-8d79f90d4ae1/jobs/spark-shell-e9c9e30c-cf40-4fc0-8c19-a903d5488ad2/',\n",
       "   u'driverInputResourceUri': u'//test-dataproc.sandbox.googleapis.com/projects/datalab-spark/clusters/0e18e110-3a94-4bf0-8651-8d79f90d4ae1/jobs/spark-shell-e9c9e30c-cf40-4fc0-8c19-a903d5488ad2/bytestreams/stdin',\n",
       "   u'driverOutputResourceUri': u'//test-dataproc.sandbox.googleapis.com/projects/datalab-spark/clusters/0e18e110-3a94-4bf0-8651-8d79f90d4ae1/jobs/spark-shell-e9c9e30c-cf40-4fc0-8c19-a903d5488ad2/bytestreams/stdout',\n",
       "   u'interactive': True,\n",
       "   u'placement': {u'clusterName': u'alekseyv-interact',\n",
       "    u'clusterUuid': u'0e18e110-3a94-4bf0-8651-8d79f90d4ae1'},\n",
       "   u'reference': {u'jobId': u'spark-shell-e9c9e30c-cf40-4fc0-8c19-a903d5488ad2',\n",
       "    u'projectId': u'datalab-spark'},\n",
       "   u'sparkJob': {u'loggingConfiguration': {},\n",
       "    u'mainClass': u'org.apache.spark.repl.Main'},\n",
       "   u'status': {u'state': u'CANCELLED',\n",
       "    u'stateStartTime': u'2015-11-23T19:35:12.843Z'},\n",
       "   u'statusHistory': [{u'state': u'PENDING',\n",
       "     u'stateStartTime': u'2015-11-21T00:20:55.018Z'},\n",
       "    {u'state': u'SETUP_DONE', u'stateStartTime': u'2015-11-21T00:20:55.267Z'},\n",
       "    {u'state': u'RUNNING', u'stateStartTime': u'2015-11-21T00:21:01.129Z'},\n",
       "    {u'state': u'CANCEL_PENDING',\n",
       "     u'stateStartTime': u'2015-11-23T19:34:42.037Z'},\n",
       "    {u'details': u'Agent reported job failure',\n",
       "     u'state': u'CANCEL_STARTED',\n",
       "     u'stateStartTime': u'2015-11-23T19:34:54.973Z'}]},\n",
       "  {u'driverControlFilesUri': u'gs://dataproc-9c5a7adf-7cdb-4eb2-b42d-d81754653069-us/google-cloud-dataproc-metainfo/0e18e110-3a94-4bf0-8651-8d79f90d4ae1/jobs/spark-shell-de00ace2-aefc-422b-95c5-64ebcd0793e4/',\n",
       "   u'driverInputResourceUri': u'//test-dataproc.sandbox.googleapis.com/projects/datalab-spark/clusters/0e18e110-3a94-4bf0-8651-8d79f90d4ae1/jobs/spark-shell-de00ace2-aefc-422b-95c5-64ebcd0793e4/bytestreams/stdin',\n",
       "   u'driverOutputResourceUri': u'//test-dataproc.sandbox.googleapis.com/projects/datalab-spark/clusters/0e18e110-3a94-4bf0-8651-8d79f90d4ae1/jobs/spark-shell-de00ace2-aefc-422b-95c5-64ebcd0793e4/bytestreams/stdout',\n",
       "   u'interactive': True,\n",
       "   u'placement': {u'clusterName': u'alekseyv-interact',\n",
       "    u'clusterUuid': u'0e18e110-3a94-4bf0-8651-8d79f90d4ae1'},\n",
       "   u'reference': {u'jobId': u'spark-shell-de00ace2-aefc-422b-95c5-64ebcd0793e4',\n",
       "    u'projectId': u'datalab-spark'},\n",
       "   u'sparkJob': {u'loggingConfiguration': {},\n",
       "    u'mainClass': u'org.apache.spark.repl.Main'},\n",
       "   u'status': {u'state': u'CANCELLED',\n",
       "    u'stateStartTime': u'2015-11-20T23:34:39.049Z'},\n",
       "   u'statusHistory': [{u'state': u'PENDING',\n",
       "     u'stateStartTime': u'2015-11-20T23:30:48.988Z'},\n",
       "    {u'state': u'SETUP_DONE', u'stateStartTime': u'2015-11-20T23:30:49.139Z'},\n",
       "    {u'state': u'RUNNING', u'stateStartTime': u'2015-11-20T23:30:55.580Z'},\n",
       "    {u'state': u'CANCEL_PENDING',\n",
       "     u'stateStartTime': u'2015-11-20T23:34:22.044Z'},\n",
       "    {u'details': u'Agent reported job failure',\n",
       "     u'state': u'CANCEL_STARTED',\n",
       "     u'stateStartTime': u'2015-11-20T23:34:28.603Z'}]},\n",
       "  {u'driverControlFilesUri': u'gs://dataproc-9c5a7adf-7cdb-4eb2-b42d-d81754653069-us/google-cloud-dataproc-metainfo/0e18e110-3a94-4bf0-8651-8d79f90d4ae1/jobs/spark-shell-e71142df-7773-4c57-89db-cb1c87bd9f1c/',\n",
       "   u'driverInputResourceUri': u'//test-dataproc.sandbox.googleapis.com/projects/datalab-spark/clusters/0e18e110-3a94-4bf0-8651-8d79f90d4ae1/jobs/spark-shell-e71142df-7773-4c57-89db-cb1c87bd9f1c/bytestreams/stdin',\n",
       "   u'driverOutputResourceUri': u'//test-dataproc.sandbox.googleapis.com/projects/datalab-spark/clusters/0e18e110-3a94-4bf0-8651-8d79f90d4ae1/jobs/spark-shell-e71142df-7773-4c57-89db-cb1c87bd9f1c/bytestreams/stdout',\n",
       "   u'interactive': True,\n",
       "   u'placement': {u'clusterName': u'alekseyv-interact',\n",
       "    u'clusterUuid': u'0e18e110-3a94-4bf0-8651-8d79f90d4ae1'},\n",
       "   u'reference': {u'jobId': u'spark-shell-e71142df-7773-4c57-89db-cb1c87bd9f1c',\n",
       "    u'projectId': u'datalab-spark'},\n",
       "   u'sparkJob': {u'loggingConfiguration': {},\n",
       "    u'mainClass': u'org.apache.spark.repl.Main'},\n",
       "   u'status': {u'state': u'CANCELLED',\n",
       "    u'stateStartTime': u'2015-11-20T23:34:52.075Z'},\n",
       "   u'statusHistory': [{u'state': u'PENDING',\n",
       "     u'stateStartTime': u'2015-11-20T23:16:47.609Z'},\n",
       "    {u'state': u'SETUP_DONE', u'stateStartTime': u'2015-11-20T23:16:47.841Z'},\n",
       "    {u'state': u'RUNNING', u'stateStartTime': u'2015-11-20T23:16:54.886Z'},\n",
       "    {u'state': u'CANCEL_PENDING',\n",
       "     u'stateStartTime': u'2015-11-20T23:34:35.499Z'},\n",
       "    {u'details': u'Agent reported job failure',\n",
       "     u'state': u'CANCEL_STARTED',\n",
       "     u'stateStartTime': u'2015-11-20T23:34:40.235Z'}]},\n",
       "  {u'driverControlFilesUri': u'gs://dataproc-9c5a7adf-7cdb-4eb2-b42d-d81754653069-us/google-cloud-dataproc-metainfo/0e18e110-3a94-4bf0-8651-8d79f90d4ae1/jobs/spark-shell-08901814-b747-41bf-8cf2-5a034a5fedf1/',\n",
       "   u'driverInputResourceUri': u'//test-dataproc.sandbox.googleapis.com/projects/datalab-spark/clusters/0e18e110-3a94-4bf0-8651-8d79f90d4ae1/jobs/spark-shell-08901814-b747-41bf-8cf2-5a034a5fedf1/bytestreams/stdin',\n",
       "   u'driverOutputResourceUri': u'//test-dataproc.sandbox.googleapis.com/projects/datalab-spark/clusters/0e18e110-3a94-4bf0-8651-8d79f90d4ae1/jobs/spark-shell-08901814-b747-41bf-8cf2-5a034a5fedf1/bytestreams/stdout',\n",
       "   u'interactive': True,\n",
       "   u'placement': {u'clusterName': u'alekseyv-interact',\n",
       "    u'clusterUuid': u'0e18e110-3a94-4bf0-8651-8d79f90d4ae1'},\n",
       "   u'reference': {u'jobId': u'spark-shell-08901814-b747-41bf-8cf2-5a034a5fedf1',\n",
       "    u'projectId': u'datalab-spark'},\n",
       "   u'sparkJob': {u'loggingConfiguration': {},\n",
       "    u'mainClass': u'org.apache.spark.repl.Main'},\n",
       "   u'status': {u'state': u'CANCELLED',\n",
       "    u'stateStartTime': u'2015-11-20T23:35:16.481Z'},\n",
       "   u'statusHistory': [{u'state': u'PENDING',\n",
       "     u'stateStartTime': u'2015-11-20T23:14:17.397Z'},\n",
       "    {u'state': u'SETUP_DONE', u'stateStartTime': u'2015-11-20T23:14:17.611Z'},\n",
       "    {u'state': u'RUNNING', u'stateStartTime': u'2015-11-20T23:14:24.826Z'},\n",
       "    {u'state': u'CANCEL_PENDING',\n",
       "     u'stateStartTime': u'2015-11-20T23:34:47.630Z'},\n",
       "    {u'details': u'Agent reported job failure',\n",
       "     u'state': u'CANCEL_STARTED',\n",
       "     u'stateStartTime': u'2015-11-20T23:34:54.369Z'}]},\n",
       "  {u'driverControlFilesUri': u'gs://dataproc-9c5a7adf-7cdb-4eb2-b42d-d81754653069-us/google-cloud-dataproc-metainfo/0e18e110-3a94-4bf0-8651-8d79f90d4ae1/jobs/spark-shell-41f019cf-8665-4b02-9939-81799e2196f4/',\n",
       "   u'driverInputResourceUri': u'//test-dataproc.sandbox.googleapis.com/projects/datalab-spark/clusters/0e18e110-3a94-4bf0-8651-8d79f90d4ae1/jobs/spark-shell-41f019cf-8665-4b02-9939-81799e2196f4/bytestreams/stdin',\n",
       "   u'driverOutputResourceUri': u'//test-dataproc.sandbox.googleapis.com/projects/datalab-spark/clusters/0e18e110-3a94-4bf0-8651-8d79f90d4ae1/jobs/spark-shell-41f019cf-8665-4b02-9939-81799e2196f4/bytestreams/stdout',\n",
       "   u'interactive': True,\n",
       "   u'placement': {u'clusterName': u'alekseyv-interact',\n",
       "    u'clusterUuid': u'0e18e110-3a94-4bf0-8651-8d79f90d4ae1'},\n",
       "   u'reference': {u'jobId': u'spark-shell-41f019cf-8665-4b02-9939-81799e2196f4',\n",
       "    u'projectId': u'datalab-spark'},\n",
       "   u'sparkJob': {u'loggingConfiguration': {},\n",
       "    u'mainClass': u'org.apache.spark.repl.Main'},\n",
       "   u'status': {u'state': u'CANCELLED',\n",
       "    u'stateStartTime': u'2015-11-20T23:14:07.456Z'},\n",
       "   u'statusHistory': [{u'state': u'PENDING',\n",
       "     u'stateStartTime': u'2015-11-20T22:43:37.207Z'},\n",
       "    {u'state': u'SETUP_DONE', u'stateStartTime': u'2015-11-20T22:43:37.383Z'},\n",
       "    {u'state': u'RUNNING', u'stateStartTime': u'2015-11-20T22:43:43.794Z'},\n",
       "    {u'state': u'CANCEL_PENDING',\n",
       "     u'stateStartTime': u'2015-11-20T23:13:49.209Z'},\n",
       "    {u'details': u'Agent reported job failure',\n",
       "     u'state': u'CANCEL_STARTED',\n",
       "     u'stateStartTime': u'2015-11-20T23:13:55.138Z'}]},\n",
       "  {u'driverControlFilesUri': u'gs://dataproc-9c5a7adf-7cdb-4eb2-b42d-d81754653069-us/google-cloud-dataproc-metainfo/0e18e110-3a94-4bf0-8651-8d79f90d4ae1/jobs/spark-shell-32fcb166-40fa-4cdc-9a9c-88fbc7803bcf/',\n",
       "   u'driverInputResourceUri': u'//test-dataproc.sandbox.googleapis.com/projects/datalab-spark/clusters/0e18e110-3a94-4bf0-8651-8d79f90d4ae1/jobs/spark-shell-32fcb166-40fa-4cdc-9a9c-88fbc7803bcf/bytestreams/stdin',\n",
       "   u'driverOutputResourceUri': u'//test-dataproc.sandbox.googleapis.com/projects/datalab-spark/clusters/0e18e110-3a94-4bf0-8651-8d79f90d4ae1/jobs/spark-shell-32fcb166-40fa-4cdc-9a9c-88fbc7803bcf/bytestreams/stdout',\n",
       "   u'interactive': True,\n",
       "   u'placement': {u'clusterName': u'alekseyv-interact',\n",
       "    u'clusterUuid': u'0e18e110-3a94-4bf0-8651-8d79f90d4ae1'},\n",
       "   u'reference': {u'jobId': u'spark-shell-32fcb166-40fa-4cdc-9a9c-88fbc7803bcf',\n",
       "    u'projectId': u'datalab-spark'},\n",
       "   u'sparkJob': {u'loggingConfiguration': {},\n",
       "    u'mainClass': u'org.apache.spark.repl.Main'},\n",
       "   u'status': {u'state': u'CANCELLED',\n",
       "    u'stateStartTime': u'2015-11-20T22:33:48.477Z'},\n",
       "   u'statusHistory': [{u'state': u'PENDING',\n",
       "     u'stateStartTime': u'2015-11-20T22:00:56.483Z'},\n",
       "    {u'state': u'SETUP_DONE', u'stateStartTime': u'2015-11-20T22:00:56.692Z'},\n",
       "    {u'state': u'RUNNING', u'stateStartTime': u'2015-11-20T22:01:27.234Z'},\n",
       "    {u'state': u'CANCEL_PENDING',\n",
       "     u'stateStartTime': u'2015-11-20T22:33:32.489Z'},\n",
       "    {u'details': u'Agent reported job failure',\n",
       "     u'state': u'CANCEL_STARTED',\n",
       "     u'stateStartTime': u'2015-11-20T22:33:41.762Z'}]},\n",
       "  {u'driverControlFilesUri': u'gs://dataproc-9c5a7adf-7cdb-4eb2-b42d-d81754653069-us/google-cloud-dataproc-metainfo/0e18e110-3a94-4bf0-8651-8d79f90d4ae1/jobs/spark-shell-1b2efdf7-0b2b-4e84-aa14-652e1c712ecd/',\n",
       "   u'driverInputResourceUri': u'//test-dataproc.sandbox.googleapis.com/projects/datalab-spark/clusters/0e18e110-3a94-4bf0-8651-8d79f90d4ae1/jobs/spark-shell-1b2efdf7-0b2b-4e84-aa14-652e1c712ecd/bytestreams/stdin',\n",
       "   u'driverOutputResourceUri': u'//test-dataproc.sandbox.googleapis.com/projects/datalab-spark/clusters/0e18e110-3a94-4bf0-8651-8d79f90d4ae1/jobs/spark-shell-1b2efdf7-0b2b-4e84-aa14-652e1c712ecd/bytestreams/stdout',\n",
       "   u'interactive': True,\n",
       "   u'placement': {u'clusterName': u'alekseyv-interact',\n",
       "    u'clusterUuid': u'0e18e110-3a94-4bf0-8651-8d79f90d4ae1'},\n",
       "   u'reference': {u'jobId': u'spark-shell-1b2efdf7-0b2b-4e84-aa14-652e1c712ecd',\n",
       "    u'projectId': u'datalab-spark'},\n",
       "   u'sparkJob': {u'loggingConfiguration': {},\n",
       "    u'mainClass': u'org.apache.spark.repl.Main'},\n",
       "   u'status': {u'state': u'CANCELLED',\n",
       "    u'stateStartTime': u'2015-11-20T22:00:15.925Z'},\n",
       "   u'statusHistory': [{u'state': u'PENDING',\n",
       "     u'stateStartTime': u'2015-11-20T20:08:40.493Z'},\n",
       "    {u'state': u'SETUP_DONE', u'stateStartTime': u'2015-11-20T20:08:41.199Z'},\n",
       "    {u'state': u'RUNNING', u'stateStartTime': u'2015-11-20T20:08:46.873Z'},\n",
       "    {u'state': u'CANCEL_PENDING',\n",
       "     u'stateStartTime': u'2015-11-20T21:59:58.598Z'},\n",
       "    {u'details': u'Agent reported job failure',\n",
       "     u'state': u'CANCEL_STARTED',\n",
       "     u'stateStartTime': u'2015-11-20T22:00:04.428Z'}]},\n",
       "  {u'driverControlFilesUri': u'gs://dataproc-9c5a7adf-7cdb-4eb2-b42d-d81754653069-us/google-cloud-dataproc-metainfo/84309d80-b7ea-4829-923f-e885a7176820/jobs/spark-shell-ad442909-e1ec-4d15-ae36-4411afb26b1b/',\n",
       "   u'driverInputResourceUri': u'//test-dataproc.sandbox.googleapis.com/projects/datalab-spark/clusters/84309d80-b7ea-4829-923f-e885a7176820/jobs/spark-shell-ad442909-e1ec-4d15-ae36-4411afb26b1b/bytestreams/stdin',\n",
       "   u'driverOutputResourceUri': u'//test-dataproc.sandbox.googleapis.com/projects/datalab-spark/clusters/84309d80-b7ea-4829-923f-e885a7176820/jobs/spark-shell-ad442909-e1ec-4d15-ae36-4411afb26b1b/bytestreams/stdout',\n",
       "   u'interactive': True,\n",
       "   u'placement': {u'clusterName': u'alekseyv-interact-test',\n",
       "    u'clusterUuid': u'84309d80-b7ea-4829-923f-e885a7176820'},\n",
       "   u'reference': {u'jobId': u'spark-shell-ad442909-e1ec-4d15-ae36-4411afb26b1b',\n",
       "    u'projectId': u'datalab-spark'},\n",
       "   u'sparkJob': {u'loggingConfiguration': {},\n",
       "    u'mainClass': u'org.apache.spark.repl.Main'},\n",
       "   u'status': {u'details': u'Task was cancelled.',\n",
       "    u'state': u'ERROR',\n",
       "    u'stateStartTime': u'2015-11-20T19:56:54.790Z'},\n",
       "   u'statusHistory': [{u'state': u'PENDING',\n",
       "     u'stateStartTime': u'2015-11-20T05:17:03.836Z'},\n",
       "    {u'state': u'SETUP_DONE', u'stateStartTime': u'2015-11-20T05:17:03.995Z'},\n",
       "    {u'state': u'RUNNING', u'stateStartTime': u'2015-11-20T05:17:16.117Z'}]},\n",
       "  {u'driverControlFilesUri': u'gs://dataproc-9c5a7adf-7cdb-4eb2-b42d-d81754653069-us/google-cloud-dataproc-metainfo/84309d80-b7ea-4829-923f-e885a7176820/jobs/spark-shell-edefbd62-c887-40f2-9aba-6440f9edc065/',\n",
       "   u'driverInputResourceUri': u'//test-dataproc.sandbox.googleapis.com/projects/datalab-spark/clusters/84309d80-b7ea-4829-923f-e885a7176820/jobs/spark-shell-edefbd62-c887-40f2-9aba-6440f9edc065/bytestreams/stdin',\n",
       "   u'driverOutputResourceUri': u'//test-dataproc.sandbox.googleapis.com/projects/datalab-spark/clusters/84309d80-b7ea-4829-923f-e885a7176820/jobs/spark-shell-edefbd62-c887-40f2-9aba-6440f9edc065/bytestreams/stdout',\n",
       "   u'interactive': True,\n",
       "   u'placement': {u'clusterName': u'alekseyv-interact-test',\n",
       "    u'clusterUuid': u'84309d80-b7ea-4829-923f-e885a7176820'},\n",
       "   u'reference': {u'jobId': u'spark-shell-edefbd62-c887-40f2-9aba-6440f9edc065',\n",
       "    u'projectId': u'datalab-spark'},\n",
       "   u'sparkJob': {u'loggingConfiguration': {},\n",
       "    u'mainClass': u'org.apache.spark.repl.Main'},\n",
       "   u'status': {u'details': u'Task was cancelled.',\n",
       "    u'state': u'ERROR',\n",
       "    u'stateStartTime': u'2015-11-20T19:56:54.591Z'},\n",
       "   u'statusHistory': [{u'state': u'PENDING',\n",
       "     u'stateStartTime': u'2015-11-20T05:14:16.549Z'},\n",
       "    {u'state': u'SETUP_DONE', u'stateStartTime': u'2015-11-20T05:14:16.784Z'},\n",
       "    {u'state': u'RUNNING', u'stateStartTime': u'2015-11-20T05:14:23.131Z'}]}],\n",
       " u'nextPageToken': u'CjBzcGFyay1zaGVsbC1lZGVmYmQ2Mi1jODg3LTQwZjItOWFiYS02NDQwZjllZGMwNjUSBgio2LqyBQ=='}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List jobs\n",
    "\n",
    "jobs_list(gcp.Context.default())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "RequestException",
     "evalue": "HTTP request failed",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRequestException\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-46334f2a8241>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[0mcontext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgcp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdefault\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgcp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_util\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_http\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mHttp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'POST'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcredentials\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgcp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdefault\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcredentials\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/gcp/_util/_http.pyc\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(url, args, data, headers, method, credentials, raw_response)\u001b[0m\n\u001b[0;32m    119\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 121\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mRequestException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    122\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Failed to process HTTP response.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRequestException\u001b[0m: HTTP request failed"
     ]
    }
   ],
   "source": [
    "# Submit interactive job\n",
    "\n",
    "data = {\n",
    "  \"job\": {\n",
    "    \"placement\": {\n",
    "      \"clusterName\": \"alekseyv-interact\"\n",
    "    },\n",
    "    \"reference\": {\n",
    "      \"jobId\": \"alekseyv-manual-004c2b1a-12bb-45fe-b2b9-a140851bc8db\",\n",
    "      \"projectId\": \"datalab-spark\"\n",
    "    },\n",
    "    \"interactive\": \"True\",\n",
    "    \"sparkJob\": {\n",
    "      \"mainClass\": \"org.apache.spark.repl.Main\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "import gcp\n",
    "import gcp._context\n",
    "import gcp._util\n",
    "import gcp._util._http\n",
    "\n",
    "url = 'https://test-dataproc.sandbox.googleapis.com/v1beta1/projects/datalab-spark/jobs:submit/'\n",
    "\n",
    "context = gcp.Context.default()\n",
    "response = gcp._util._http.Http.request(url, method='POST', data=data, credentials=gcp.Context.default().credentials)\n",
    "response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'driverControlFilesUri': u'gs://dataproc-9c5a7adf-7cdb-4eb2-b42d-d81754653069-us/google-cloud-dataproc-metainfo/888c53bb-3ab9-4e9b-8433-5a6730857535/jobs/alekseyv-manual-004c2b1a-12bb-45fe-b2b9-a140851bc8da/',\n",
       " u'driverInputResourceUri': u'//test-dataproc.sandbox.googleapis.com/projects/datalab-spark/clusters/888c53bb-3ab9-4e9b-8433-5a6730857535/jobs/alekseyv-manual-004c2b1a-12bb-45fe-b2b9-a140851bc8da/bytestreams/stdin',\n",
       " u'driverOutputResourceUri': u'//test-dataproc.sandbox.googleapis.com/projects/datalab-spark/clusters/888c53bb-3ab9-4e9b-8433-5a6730857535/jobs/alekseyv-manual-004c2b1a-12bb-45fe-b2b9-a140851bc8da/bytestreams/stdout',\n",
       " u'interactive': True,\n",
       " u'placement': {u'clusterName': u'alekseyv-interact',\n",
       "  u'clusterUuid': u'888c53bb-3ab9-4e9b-8433-5a6730857535'},\n",
       " u'reference': {u'jobId': u'alekseyv-manual-004c2b1a-12bb-45fe-b2b9-a140851bc8da',\n",
       "  u'projectId': u'datalab-spark'},\n",
       " u'sparkJob': {u'loggingConfiguration': {},\n",
       "  u'mainClass': u'org.apache.spark.repl.Main'},\n",
       " u'status': {u'state': u'CANCELLED',\n",
       "  u'stateStartTime': u'2015-11-09T21:16:25.559Z'},\n",
       " u'statusHistory': [{u'state': u'PENDING',\n",
       "   u'stateStartTime': u'2015-11-06T23:32:18.658Z'},\n",
       "  {u'state': u'SETUP_DONE', u'stateStartTime': u'2015-11-06T23:32:18.825Z'},\n",
       "  {u'state': u'RUNNING', u'stateStartTime': u'2015-11-06T23:32:24.538Z'},\n",
       "  {u'state': u'CANCEL_PENDING',\n",
       "   u'stateStartTime': u'2015-11-07T01:14:12.880Z'},\n",
       "  {u'details': u'Agent reported job failure',\n",
       "   u'state': u'CANCEL_STARTED',\n",
       "   u'stateStartTime': u'2015-11-09T21:16:19.719Z'}]}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get interactive job details\n",
    "\n",
    "job_id = \"alekseyv-manual-004c2b1a-12bb-45fe-b2b9-a140851bc8da\"\n",
    "\n",
    "url = 'https://test-dataproc.sandbox.googleapis.com/v1beta1/projects/datalab-spark/jobs/' + job_id\n",
    "\n",
    "context = gcp.Context.default()\n",
    "job_details_response = gcp._util.Http.request(url, '', credentials=gcp.Context.default().credentials)  \n",
    "job_details_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gcp.Context.default().credentials.refresh(None)\n",
    "gcp.Context._global_context = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/etc/ssl/certs/ca-certificates.crt\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "ls /etc/ssl/certs/ca-*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ya29.TALmGVpmOro0LtUn006y1ppMRiZlOBSHyGBcx1IDQ8qJvZPVYEWIZft0nB8IuSj_BzVz7v8\n",
      "response: data: \"e.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\\n15/12/09 04:09:12 INFO hive.metastore: Trying to connect to metastore with URI thrift://alekseyv-interact-m:9083\\n15/12/09 04:09:12 INFO hive.metastore: Connected to metastore.\\n15/12/09 04:09:13 WARN org.apache.hadoop.hdfs.shortcircuit.DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.\\n15/12/09 04:09:13 INFO org.apache.hadoop.hive.ql.session.SessionState: Created local directory: /tmp/2cf002f5-1605-4149-9632-55052ef663b1_resources\\n15/12/09 04:09:13 INFO org.apache.hadoop.hive.ql.session.SessionState: Created HDFS directory: /tmp/hive/root/2cf002f5-1605-4149-9632-55052ef663b1\\n15/12/09 04:09:13 INFO org.apache.hadoop.hive.ql.session.SessionState: Created local directory: /tmp/root/2cf002f5-1605-4149-9632-55052ef663b1\\n15/12/09 04:09:13 INFO org.apache.hadoop.hive.ql.session.SessionState: Created HDFS directory: /tmp/hive/ro\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gcp\n",
    "import gcp._context\n",
    "import gcp._util\n",
    "\n",
    "import sys\n",
    "import grpc.framework.face.exceptions\n",
    "from grpc.beta import implementations\n",
    "from gcp.spark.bytestream_pb2 import beta_create_ByteStream_stub\n",
    "from gcp.spark.bytestream_pb2 import ReadRequest\n",
    "from gcp.spark.bytestream_pb2 import ReadResponse\n",
    "from oauth2client.client import GoogleCredentials\n",
    "from grpc.beta.implementations import ClientCredentials\n",
    "\n",
    "class MetadataTransformer(object):\n",
    "    \"\"\"Callable class to transform metadata for gRPC requests.\n",
    "    :type client: :class:`.client.Client`\n",
    "    :param client: The client that owns the cluster. Provides authorization and\n",
    "                   user agent.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, client):\n",
    "        self._credentials = client.credentials\n",
    "        self._user_agent = 'GoogleCloudDataLab/1.0'\n",
    "\n",
    "    def __call__(self, ignored_val):\n",
    "        \"\"\"Adds authorization header to request metadata.\"\"\"\n",
    "        access_token = self._credentials.get_access_token().access_token\n",
    "        print(access_token)\n",
    "        return [\n",
    "            ('Authorization', 'Bearer ' + access_token),\n",
    "            ('User-agent', self._user_agent),\n",
    "        ]\n",
    "\n",
    "SSL_CERT_FILE = '/etc/ssl/certs/ca-certificates.crt'\n",
    "def get_certs():\n",
    "    \"\"\"Gets the root certificates.\n",
    "    .. note::\n",
    "        This is only called by :func:`make_stub`. For most applications,\n",
    "        a few gRPC stubs (four total, one for each service) will be created\n",
    "        when a :class:`.Client` is created. This function will not likely\n",
    "        be used again while that application is running.\n",
    "        However, it may be worthwhile to cache the output of this function.\n",
    "    :rtype: str\n",
    "    :returns: The root certificates for the current machine.\n",
    "    \"\"\"\n",
    "    with open(SSL_CERT_FILE, mode='rb') as file_obj:\n",
    "        return file_obj.read()\n",
    "\n",
    "# refresh OAuth token\n",
    "#gcp.Context.default().credentials.refresh(None)\n",
    "#gcp.Context._global_context = None\n",
    "\n",
    "DEFAULT_HOST = \"test-dataproc.sandbox.googleapis.com\";\n",
    "DEFAULT_PORT = 443;\n",
    "\n",
    "JOB_ID = 'spark-shell-92ec3127-5254-40d0-aaf6-077c3833e906'\n",
    "CLUSTER_ID = '39d18ff1-db0d-4305-bfb1-66dfd69e5758'\n",
    "\n",
    "driverInputResourceUri = u'//test-dataproc.sandbox.googleapis.com/projects/datalab-spark/clusters/%s/jobs/%s/bytestreams/stdin' % (CLUSTER_ID, JOB_ID)\n",
    "driverOutputResourceUri = u'//test-dataproc.sandbox.googleapis.com/projects/datalab-spark/clusters/%s/jobs/%s/bytestreams/stdout' % (CLUSTER_ID, JOB_ID)\n",
    "\n",
    "custom_metadata_transformer = MetadataTransformer(gcp.Context.default())\n",
    "\n",
    "root_certificates = get_certs()\n",
    "client_credentials = implementations.ssl_client_credentials(root_certificates, private_key=None, certificate_chain=None)\n",
    "\n",
    "try:\n",
    "  channel = implementations.secure_channel(DEFAULT_HOST, DEFAULT_PORT, client_credentials)\n",
    "  stub = beta_create_ByteStream_stub(channel, metadata_transformer=custom_metadata_transformer)\n",
    "  readRequest = ReadRequest(resource_name=driverOutputResourceUri, read_limit=1024, read_offset=2900)\n",
    "\n",
    "  response = stub.Read(readRequest, 10)\n",
    "  print('response: %s' % response.result())\n",
    "except grpc.framework.interfaces.face.face.ExpirationError as ex:\n",
    "  print('expiration error: ex.code:%s, ex.details:%s' % (ex.code, ex.details))\n",
    "except grpc.framework.interfaces.face.face.NetworkError as ex:\n",
    "  print('expiration error: ex.code:%s, ex.details:%s' % (ex.code, ex.details))  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
