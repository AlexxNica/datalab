{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting new spark_sql_proxy_job\n",
      "started new spark_sql_proxy_job, jobId:alekseyv-spark-sql-proxy-12345-40acdddf-ff3e-4890-b587-6d809a9d1660\n",
      "waiting for job alekseyv-spark-sql-proxy-12345-40acdddf-ff3e-4890-b587-6d809a9d1660 to start, current state SETUP_DONE, retry 0\n",
      "setup for spark_sql_proxy_job is complete, jobId:alekseyv-spark-sql-proxy-12345-40acdddf-ff3e-4890-b587-6d809a9d1660\n",
      "connecting to spark_sql_proxy_job socket\n",
      "connected to spark_sql_proxy_job socket\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gcp\n",
    "import gcp._context\n",
    "import gcp._util\n",
    "import json\n",
    "import os\n",
    "import uuid\n",
    "import time\n",
    "import socket\n",
    "\n",
    "class spark_sql_client:\n",
    "  _V1_BETA1_ENDPOINT = 'https://dataproc.googleapis.com/v1beta1'\n",
    "  _CLUSTERS_PATH = '/projects/{project_id}/clusters/{cluster_name}'\n",
    "  _JOB_PATH = '/projects/{project_id}/jobs'\n",
    "  _JOBS_PATH = _JOB_PATH + '/{job_id}'\n",
    "  \n",
    "  _V1_ENDPOINT = 'https://www.googleapis.com/compute/v1'\n",
    "  _INSTANCES_PATH = '/projects/{project_id}/zones/{zone}/instances/{instance}'\n",
    "  _PORT = 12345\n",
    "  \n",
    "  def __init__(self, context):\n",
    "    #TODO: move cluster_name to constructor\n",
    "    self.is_connected = False\n",
    "    self._credentials = context.credentials\n",
    "    self._project_id = context.project_id\n",
    "    self._socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "    self._master_node_ip = None\n",
    "    self._job_id = None\n",
    "\n",
    "  def get_cluster(self, cluster_name):\n",
    "    url = spark_sql_client._V1_BETA1_ENDPOINT + spark_sql_client._CLUSTERS_PATH.format(project_id=self._project_id, cluster_name=cluster_name)\n",
    "    return gcp._util.Http.request(url, '', credentials=self._credentials)  \n",
    "\n",
    "  def get_compute_instance(self, instance_name, zone):\n",
    "    url = spark_sql_client._V1_ENDPOINT + spark_sql_client._INSTANCES_PATH.format(project_id=self._project_id, zone=zone, instance = instance_name)\n",
    "    return gcp._util.Http.request(url, '', credentials=self._credentials)  \n",
    "    \n",
    "  def get_cluster_master_node_ip(self, cluster_name):\n",
    "    cluster = self.get_cluster(cluster_name)\n",
    "    \n",
    "    if cluster == None:\n",
    "      return None\n",
    "    \n",
    "    zone_uri = cluster['configuration']['gceClusterConfiguration']['zoneUri']\n",
    "    zone = zone_uri[zone_uri.find('/zones/') + len('/zones/'):]\n",
    "    \n",
    "    compute_instance = self.get_compute_instance(cluster_name + '-m', zone)\n",
    "    if 'networkInterfaces' in compute_instance and len(compute_instance['networkInterfaces']) > 0:\n",
    "      network_interface = compute_instance['networkInterfaces'][0]\n",
    "      if 'accessConfigs' in network_interface and len(network_interface['accessConfigs']) > 0:\n",
    "        accessConfig = network_interface['accessConfigs'][0]\n",
    "        return accessConfig['natIP'] # TODO(alekseyv): figure out how to find correct IP if there is more than one accessConfig\n",
    "    return None\n",
    "  \n",
    "  def get_user_name(self):\n",
    "    # TODO: not sure if it'll work when deployed to cloud, figure out proper way to find user.\n",
    "    user_name = os.environ['DATALAB_USER']\n",
    "    if user_name is None:\n",
    "      return user_name\n",
    "    else:\n",
    "      index = user_name.find('@')\n",
    "      if index > 0:\n",
    "        return user_name[:index]\n",
    "      \n",
    "  def jobs_list(self):\n",
    "    url = spark_sql_client._V1_BETA1_ENDPOINT + '/projects/' + self._project_id + '/jobs/'\n",
    "    return gcp._util.Http.request(url, '', credentials=self._credentials)\n",
    "    \n",
    "  def cancel_job(self, job_id):\n",
    "    url = spark_sql_client._V1_BETA1_ENDPOINT + spark_sql_client._JOBS_PATH.format(project_id = self._project_id, job_id=job_id) + \":cancel\"\n",
    "    print(url)\n",
    "    return gcp._util._http.Http.request(url, method='POST', credentials=self._credentials, raw_response=True)\n",
    "    \n",
    "  def get_spark_sql_proxy_job_name_prefix(self):\n",
    "    return self.get_user_name() + \"-spark-sql-proxy-\" + str(spark_sql_client._PORT) + \"-\"\n",
    "  \n",
    "  def submit_spark_job(self, cluster_name):\n",
    "    job_id = self.get_spark_sql_proxy_job_name_prefix() + str(uuid.uuid4())\n",
    "    data = {\n",
    "      \"projectId\": self._project_id,\n",
    "      \"job\": {\n",
    "        \"placement\": {\n",
    "          \"clusterName\": cluster_name\n",
    "        },\n",
    "        \"reference\": {\n",
    "          \"jobId\": job_id,\n",
    "          \"projectId\": self._project_id\n",
    "        },\n",
    "        \"sparkJob\": {\n",
    "          \"args\" : [str(spark_sql_client._PORT)],\n",
    "          \"jarFileUris\": [\"gs://alekseyv-test/SparkSqlProxyServer.jar\"],\n",
    "          \"mainClass\": \"SparkSqlProxyServer\"\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "    url = spark_sql_client._V1_BETA1_ENDPOINT + spark_sql_client._JOB_PATH.format(project_id = self._project_id) + \":submit\"\n",
    "    gcp._util._http.Http.request(url, data=data, credentials=self._credentials, raw_response=True)\n",
    "    return job_id\n",
    "  \n",
    "  def get_job(self, job_id):\n",
    "    url = spark_sql_client._V1_BETA1_ENDPOINT + spark_sql_client._JOBS_PATH.format(project_id=self._project_id, job_id=job_id)\n",
    "    return gcp._util.Http.request(url, '', credentials=self._credentials)\n",
    "  \n",
    "  def try_get_running_spark_sql_proxy_job(self):\n",
    "    jobs = self.jobs_list()\n",
    "    if not('jobs' in jobs):\n",
    "      return None\n",
    "    running_spark_sql_proxy_jobs = filter(lambda job:\\\n",
    "                                          job['reference']['jobId'].startswith(self.get_spark_sql_proxy_job_name_prefix()) and \\\n",
    "                                          job['status']['state'] == 'RUNNING', jobs['jobs'])\n",
    "    if len(running_spark_sql_proxy_jobs) == 0:\n",
    "      return None\n",
    "    if len(running_spark_sql_proxy_jobs) > 1:\n",
    "      print('WARNING: there are more than one spark_sql_proxy_jobs, returning the first one')\n",
    "    return running_spark_sql_proxy_jobs[0]\n",
    "    \n",
    "  def wait_for_job_to_start(self, job_id):\n",
    "    _POLLING_INTERVAL_SECONDS = 10\n",
    "    _MAX_RETRY_COUNT = 6\n",
    "    retry_count = 0\n",
    "    while retry_count < _MAX_RETRY_COUNT:\n",
    "      job = self.get_job(job_id)\n",
    "      if job is None:\n",
    "        return False\n",
    "      job_state = job['status']['state']\n",
    "      if job_state == 'RUNNING':# or job_state == 'SETUP_DONE': # it looks like SETUP_DONE state is not correct, check state/status\n",
    "        return True\n",
    "      if job_state == 'ERROR' or job_state == 'CANCELLED' or job_state == 'DONE':\n",
    "        return False\n",
    "      time.sleep(_POLLING_INTERVAL_SECONDS)\n",
    "      print(\"waiting for job {job_id} to start, current state {job_state}, retry {retry}\".format(job_id = job_id, job_state = job_state, retry = retry_count))\n",
    "      retry_count += 1\n",
    "    return False\n",
    "    \n",
    "  def connect(self, cluster_name):\n",
    "    if self.is_connected:\n",
    "      return True\n",
    "    \n",
    "    if self._master_node_ip is None:\n",
    "      self._master_node_ip = self.get_cluster_master_node_ip(cluster_name)\n",
    "      if self._master_node_ip is None:\n",
    "        return False\n",
    "      if self._job_id is None:\n",
    "        running_job = self.try_get_running_spark_sql_proxy_job()\n",
    "        if running_job is not None:\n",
    "          self._job_id = running_job['reference']['jobId']\n",
    "          print('found running spark_sql_proxy_job, jobId:' + self._job_id)\n",
    "        else:\n",
    "          print('starting new spark_sql_proxy_job')\n",
    "          self._job_id = self.submit_spark_job(cluster_name)\n",
    "          print('started new spark_sql_proxy_job, jobId:' + self._job_id)\n",
    "          if self._job_id is None:\n",
    "            return False\n",
    "          if not self.wait_for_job_to_start(self._job_id):\n",
    "            print('failed to start new spark_sql_proxy_job, jobId:' + self._job_id)\n",
    "            return False\n",
    "          print('setup for spark_sql_proxy_job is complete, jobId:' + self._job_id)\n",
    "    print('connecting to spark_sql_proxy_job socket')\n",
    "    time.sleep(20) # TODO: it is not clear how much time it takes for job to initialize, try to connect to socket by polling\n",
    "    self._socket.connect((self._master_node_ip, spark_sql_client._PORT))\n",
    "    self.is_connected = True\n",
    "    print('connected to spark_sql_proxy_job socket')\n",
    "    return True\n",
    "  \n",
    "  def disconnect_and_stop_spark_sql_proxy_job(self):\n",
    "    if not self.is_connected or self._socket is None:\n",
    "      return\n",
    "    self._socket.close()\n",
    "    if self._job_id is None:\n",
    "      return\n",
    "    self.cancel_job(self._job_id)\n",
    "    \n",
    "  def send(self, msg):\n",
    "      totalsent = 0\n",
    "      while totalsent < len(msg):\n",
    "          sent = self._socket.send(msg[totalsent:])\n",
    "          if sent == 0:\n",
    "              raise RuntimeError(\"socket connection broken\")\n",
    "          totalsent = totalsent + sent\n",
    "\n",
    "  def readlines(self, recv_buffer=4096, delim='\\n'):\n",
    "    buffer = ''\n",
    "    data = True\n",
    "    while data:\n",
    "      data = self._socket.recv(recv_buffer)\n",
    "      buffer += data\n",
    "\n",
    "      while buffer.find(delim) != -1:\n",
    "          line, buffer = buffer.split('\\n', 1)\n",
    "          yield line\n",
    "      return\n",
    "\n",
    "  def execute_statement(self, statement):\n",
    "    if not self.is_connected:\n",
    "      return\n",
    "    if not statement.endswith('\\n'):\n",
    "      statement += '\\n'\n",
    "    self.send(statement)\n",
    "    for line in client.readlines():\n",
    "      print(line)\n",
    "  \n",
    "    \n",
    "client = spark_sql_client(gcp.Context.default())\n",
    "client.connect('cluster-1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*end*\n",
      "[null,Michael]\n",
      "[30,Andy]\n",
      "[19,Justin]\n",
      "*end*\n"
     ]
    }
   ],
   "source": [
    "\n",
    "client.execute_statement(\"create temporary table people_json3 using org.apache.spark.sql.json options (path 'gs://alekseyv-test/people.json')\")\n",
    "client.execute_statement(\"select * from people_json3\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://dataproc.googleapis.com/v1beta1/projects/datalab-spark/jobs/alekseyv-spark-sql-proxy-12345-abddf2f0-c40c-4fb3-b848-436808712c72:cancel\n"
     ]
    }
   ],
   "source": [
    "client.disconnect_and_stop_spark_sql_proxy_job()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
